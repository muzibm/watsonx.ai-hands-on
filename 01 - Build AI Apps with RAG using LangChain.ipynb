{"cells": [{"metadata": {}, "id": "e53d32ab", "cell_type": "markdown", "source": "# 1. Build AI Apps with RAG using watsonx.ai, LangChain & Vector DB"}, {"metadata": {}, "id": "c64d075d", "cell_type": "markdown", "source": "## Overview\n\nIn this hands-on, you will use LangChain, a framework for building LLM applications.\nYou will learn about:\n- 1.1 Simple Prompt to LLM using LangChain\n- 1.2 Zero-Shot Prompt and Few-Shot Prompt using Prompt Template\n- 1.3 Sequential Prompts using Simple Sequential Chain\n- 1.4 Retrieval Question Answering (QA)\n- 1.5 Documents Summarization"}, {"metadata": {}, "id": "36c33201", "cell_type": "markdown", "source": "## 1.1 Simple Prompt to LLM using LangChain\n- Basic use case of sending prompts to LLM in watsonx (without using Langchain). \n- In this example, we are sending a simple prompt directly to the LLM model (Google flan-ul2)."}, {"metadata": {"scrolled": true}, "id": "1664a584", "cell_type": "code", "source": "# Install library\n!pip install chromadb==0.4.2\n!pip install langchain==0.0.312\n!pip install langchain --upgrade\n!pip install flask-sqlalchemy --user\n!pip install pypdf \n!pip install sentence-transformers\n!pip install langchain_openai", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "id": "4adcdb35", "cell_type": "code", "source": "# Import libraries\nimport os\nimport warnings\n\n#from dotenv import load_dotenv\nfrom time import sleep\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\nfrom langchain import PromptTemplate # Langchain Prompt Template\nfrom langchain.chains import LLMChain, SimpleSequentialChain # Langchain Chains\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\nfrom langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\nfrom langchain.text_splitter import CharacterTextSplitter # Text splitter\n\nwarnings.filterwarnings(\"ignore\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "96339420", "cell_type": "code", "source": "# Get API key and URL from .env\n#load_dotenv()\napi_key = \"<YOUR API KEY HERE>\"\nibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\nproject_id = \"<YOUR PROJECT ID HERE>\"\n\nif api_key is None or ibm_cloud_url is None or project_id is None:\n    raise Exception(\"One or more environment variables are missing!\")\nelse:\n    creds = {\n        \"url\": ibm_cloud_url,\n        \"apikey\": api_key \n    }", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "a51cbd27", "cell_type": "code", "source": "# Initialize the watsonx model\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.2,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 25,\n    GenParams.REPETITION_PENALTY: 1.0,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 20\n}\n\nllm_model = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n)\n\nprint(\"Done initializing LLM.\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "1b25c008", "cell_type": "code", "source": "# Send a simple prompt to model\ncountries = [\"France\", \"Japan\", \"Australia\"]\n\ntry:\n  for country in countries:\n    question = f\"What is the capital of {country}\"\n    res = llm_model.generate_text(question)\n    print(f\"The capital of {country} is {res.capitalize()}\")\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "0bf74712", "cell_type": "markdown", "source": "## 1.2 Zero-Shot Prompt and Few-Shot Prompt using Prompt Template\n- Real use case can be more complex. Instead of sending plain prompts to LLM, we are using Langchain Prompt Template. \n- In this example, we are using Langchain Prompt Template to send prompt to the LLM model (Google flan-ul2).\n- Advantags of using Prompt Template:\n    1. **Modularity**: With a prompt template, you can define a structured template once and reuse it with different input variables. This makes your code more modular and easier to maintain.\n    2. **Dynamic Input**: Prompt templates allow for dynamic input variables, such as \"country\" in this example. This means you can easily change the input value without modifying the entire prompt structure.\n    3. **Readability**: Templates provide a clear structure for the prompt, making it easier for other developers to understand the purpose of the prompt and how it interacts with the model.\n    4. **Flexibility**: You can customize the template to suit your specific use case or domain requirements. This flexibility enables you to adapt the prompt to different scenarios without rewriting the entire prompt logic."}, {"metadata": {}, "id": "2c92d3c2", "cell_type": "markdown", "source": "## Zero-shot Prompt\n- Zero-shot prompt is the simplest type of prompt. It provides no examples to the model, just the instruction. \n- You can phrase the instruction as a question. i.e: *\"Explain the concept of Generative AI.\"*\n- You can also give the model a 'role'. i.e: *\"You are a Data Scientist. Explain the concept of Generative AI.\"*"}, {"metadata": {"scrolled": true}, "id": "7b140fb9", "cell_type": "code", "source": "# Define the prompt template\nprompt = PromptTemplate(\n  input_variables=[\"country\"],\n  template= \"What is the capital of {country}?\",\n)\n\ntry:\n  # In order to use Langchain, we need to instantiate Langchain extension\n  lc_llm_model = WatsonxLLM(model=llm_model)\n  \n  # Define a chain based on model and prompt\n  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n\n  # Getting predictions\n  countries = [\"France\", \"Japan\", \"Australia\"]\n  for country in countries:\n    response = chain.run(country)\n    print(prompt.format(country=country) + \" = \" + response.capitalize())\n    sleep(0.5)\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "e52a5340", "cell_type": "markdown", "source": "## Few-shot Prompt\n- Few-shot prompt is giving the model a few examples to figure out how to handle similar task in the future.\n- It helps the model understand the task better."}, {"metadata": {}, "id": "572caefb", "cell_type": "code", "source": "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n\n# Few -shot examples\nexamples = [\n    {\"input\": \"What is the capital of Sweden?\", \"output\": \"Stockholm\"},\n    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"Kuala Lumpur\"},\n]\n\nexample_prompt = ChatPromptTemplate.from_messages(\n    [('human', '{input}'), ('ai', '{output}')]\n)\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n)\n\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [\n        #('system', 'You are a helpful AI Assistant'),\n        few_shot_prompt,\n        ('human', '{input}'),\n    ]\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "57575ce2", "cell_type": "code", "source": "try:\n  # In order to use Langchain, we need to instantiate Langchain extension\n  lc_llm_model = WatsonxLLM(model=llm_model)\n  \n  # Define a chain based on model and prompt\n  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n\n  # Getting predictions\n  countries = [\"France\", \"Japan\", \"Australia\"]\n  for country in countries:\n    prompt = f\"What is the capital of {country}?\"\n    print(prompt)\n    response = chain.run(prompt)\n    print(response)\n    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n    sleep(0.5)\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "ab2d56c4", "cell_type": "code", "source": "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n\n# Few -shot examples\nexamples = [\n    {\"input\": \"What is the capital of Sweden?\", \"output\": \"The capital of Sweden is Stockholm\"},\n    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"The capital of Malaysia is Kuala Lumpur\"},\n]\n\nexample_prompt = ChatPromptTemplate.from_messages(\n    [('human', '{input}'), ('ai', '{output}')]\n)\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n)\n\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [\n        #('system', 'You are a helpful AI Assistant'),\n        few_shot_prompt,\n        ('human', '{input}'),\n    ]\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "aeb4bb8c", "cell_type": "code", "source": "try:\n  # In order to use Langchain, we need to instantiate Langchain extension\n  lc_llm_model = WatsonxLLM(model=llm_model)\n  \n  # Define a chain based on model and prompt\n  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n\n  # Getting predictions\n  countries = [\"France\", \"Japan\", \"Australia\"]\n  for country in countries:\n    prompt = f\"What is the capital of {country}?\"\n    print(prompt)\n    response = chain.run(prompt)\n    print(response)\n    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n    sleep(0.5)\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "6f4674b8", "cell_type": "markdown", "source": "## 1.3 Sequential Prompts using Simple Sequential Chain\n- By using Simple Sequential Chain in LangChain, you can easily chain multiple prompts to create sequential prompts.\n- Prompt chaining, also known as Sequential prompts, enables the response to one prompt to become the input for the next prompt in the sequence.\n- Each subsequent prompt is informed by the AI's previous response, creating a chain of interactions that progressively refines the model's output.\n- Reference: [SimpleSequentialChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html)"}, {"metadata": {}, "id": "9e999911", "cell_type": "code", "source": "# Create two sequential prompts \npt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\npt2 = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"Answer the following question: {question}\",\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "5f4aabdc", "cell_type": "code", "source": "# Instantiate 2 models (Note, these could be different models depending on use case)\n# Note the .to_langchain() method which returns a WatsonxLLM wrapper, like above.\nmodel_1 = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()\n\nmodel_2 = Model(\n    model_id=\"google/flan-ul2\",\n    credentials=creds,\n    project_id=project_id\n).to_langchain()", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "833f356c", "cell_type": "code", "source": "# Construct the sequential chain\nprompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\nprompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\nqa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "id": "ed6495db", "cell_type": "code", "source": "# Run our chain with the topic: \"an animal\"\n# Play around with providing different topics to see the output. eg. cars, the Roman empire\ntry:\n  qa.run(\"an animal\")\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "ffd84ba9", "cell_type": "markdown", "source": "## 1.4 Retrieval Question Answering (QA)\n- Using Retrieval Question Answering (QA) in LangChain, you can easily extract passages from documents as answers to your prompt (Question). \n- To begin, download a sample pdf file from this link: [what_is_generative_ai.pdf](https://ibm.box.com/v/what-is-generative-ai)\n- Then, upload your file to Project and create the access token."}, {"metadata": {}, "id": "82e7723f", "cell_type": "code", "source": "# Import library\nfrom ibm_watson_studio_lib import access_project_or_space\nfrom langchain.chains import RetrievalQA", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "edc8757d", "cell_type": "code", "source": "# Create access token in project\ntoken = \"<YOUR ACCESS TOKEN HERE>\"\nwslib = access_project_or_space({\"token\":token})\nwslib.download_file(\"what_is_generative_ai.pdf\")", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "id": "839d0893", "cell_type": "code", "source": "# Load PDF document\npdf = 'what_is_generative_ai.pdf'\nloaders = [PyPDFLoader(pdf)]", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "id": "62bb2f6e", "cell_type": "code", "source": "# Index loaded PDF\nindex = VectorstoreIndexCreator(\n    embedding = HuggingFaceEmbeddings(),\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "e723ff88", "cell_type": "code", "source": "# Initialize watsonx google/flan-ul2 model\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.2,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 100,\n    GenParams.MIN_NEW_TOKENS: 50,\n    GenParams.MAX_NEW_TOKENS: 300\n}\n\nmodel = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "997ae488", "cell_type": "code", "source": "# Initialize RAG chain\nchain = RetrievalQA.from_chain_type(llm=model, \n                                    chain_type=\"stuff\", \n                                    retriever=index.vectorstore.as_retriever(), \n                                    input_key=\"question\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "d4891cb0", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What is Machine Learning?\")\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "20564da9", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What are the problems generative AI can solve?\")\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "a3ab61fd", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What are the risks of Generative AI?\")\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "7187e120", "cell_type": "markdown", "source": "## 1.5 Documents Summarization\n- Text summarization is a task in NLP that makes short but informative summaries of long texts. LLM can be used to make summaries of news articles, research papers, technical documents, and other kinds of text.\n- Summarizing long documents can be challenging. To generate summaries, you need to apply summarization strategies on your indexed documents. \n- In this example, we will summarize long documents from these 3 websites:\n     - https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\n     - https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/\n     - https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\n- When building a summarizer app, these are methods to pass your documents into the LLM\u2019s context window:\n    1. **Method 1: Stuff** - Simply \u201cstuff\u201d all documents into a single prompt. (Simplest method)\n    2. **Method 2: MapReduce** - Summarize each document on it\u2019s own in a \u201cmap\u201d step and then \u201creduce\u201d the summaries into a final summary."}, {"metadata": {"scrolled": true}, "id": "c551b02c", "cell_type": "code", "source": "# Install library\n!pip3 install transformers chromadb langchain", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "b8903f79", "cell_type": "code", "source": "# Import libraries\nimport os\nfrom dotenv import load_dotenv\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.chains.summarize import load_summarize_chain\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "0eaaddb9", "cell_type": "markdown", "source": "## Method 1: Stuff\n- This method simply \u201cstuff\u201d all documents into a single prompt.\n- What you need to do is setting `stuff` as `chain_type` of your chain."}, {"metadata": {}, "id": "915fcefa", "cell_type": "markdown", "source": "### Stuff without using Prompt Template\n- Prompt and LLMs pipeline is wrapped in a single object: `load_summarize_chain`.\n- Set `stuff` as the `chain_type`.\n- In this example, you will see that the relatively short document will be summarized successfully."}, {"metadata": {"scrolled": false}, "id": "3dba2d0a", "cell_type": "code", "source": "# Initialize document loader\nloader = WebBaseLoader(\"https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\")\ndoc = loader.load()\n\n# Initialize watsonx google/flan-t5-xxl model\n# You might need to tweak some of the runtime parameters to optimize the results\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.15,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 20,\n    GenParams.REPETITION_PENALTY: 1.0,\n    GenParams.MIN_NEW_TOKENS: 20,\n    GenParams.MAX_NEW_TOKENS: 205\n}\n\nflan_model = Model(\n    model_id=\"google/flan-t5-xxl\", \n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()\n\n# Set chain_type as 'stuff'\nchain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n\n# Run summarization task\nres = chain.run(doc)\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "eeb53a81", "cell_type": "markdown", "source": "### Stuff using Prompt Template\n- You will load the document into a prompt template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well.\n- `StuffDocumentsChain` will be used as part of the `load_summarize_chain` method.\n- In this example, you will see the same summarization output as above.\n- Reference: [StuffDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.chains.combine_documents.stuff.StuffDocumentsChain)"}, {"metadata": {"scrolled": true}, "id": "6b1c8e1a", "cell_type": "code", "source": "#Import librararies\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\n\n# Define prompt\nprompt_template = \"\"\"Write a concise summary of the following:\n\"{text}\"\nCONCISE SUMMARY:\"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\n\n# Define LLMs chain\nllm_chain = LLMChain(llm=flan_model, prompt=prompt)\n\n# Define StuffDocumentsChain\nstuff_chain = StuffDocumentsChain(\n    llm_chain=llm_chain, document_variable_name=\"text\"\n)\n\n# Run summarization task \nres = stuff_chain.run(doc)\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "2938eb51", "cell_type": "markdown", "source": "### Limitation of 'Stuff' Method due to LLMs token limit\n- In this example, you will see that as we add more documents (which increase the tokens), this error will be raised: `the number of input tokens 5222 cannot exceed the total tokens limit 4096 for this model`\n- This is due to the token limit for the model (Max context window length). \n- With LangChain, this can be worked around by using `MapReduce` which execute chunking and recursive summarization method."}, {"metadata": {}, "id": "43c2879b", "cell_type": "code", "source": "# Load a new document from URL\nloader_2 = WebBaseLoader('https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/')\ndoc_2 = loader_2.load()\n\n# Combine the new document to the previous document\ndocs = doc + doc_2\n\n# Run the stuff chain\ntry:\n  res = stuff_chain.run(docs)\n  print(res)\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "26d3f5e1", "cell_type": "markdown", "source": "## Method 2: MapReduce\n- This method summarize each document on it\u2019s own in a \u201cmap\u201d step and then \u201creduce\u201d the summaries into a final summary.\n- Reference: [ReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html#langchain.chains.combine_documents.reduce.ReduceDocumentsChain)\n- Reference: [MapReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain)"}, {"metadata": {}, "id": "afc5688e", "cell_type": "code", "source": "from transformers import AutoTokenizer\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\nfrom time import perf_counter\n\n# Add a 3rd document\nprint(\"Loading 3rd document...\")\nloader_3 = WebBaseLoader(\"https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\")\ndoc_3 = loader_3.load()\ndocs = docs + doc_3\n\n# Map\nmap_template = \"\"\"The following is a set of documents\n{docs}\nBased on this list of docs, please identify the main themes \nHelpful Answer:\"\"\"\nmap_prompt = PromptTemplate.from_template(map_template)\nprint(\"Init map chain...\")\nmap_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n\n# Reduce\nreduce_template = \"\"\"The following is set of summaries:\n{doc_summaries}\nTake these and distill it into a final, consolidated summary of the main themes. \nHelpful Answer:\"\"\"\nreduce_prompt = PromptTemplate.from_template(reduce_template)\nprint(\"Init reduce chain...\")\nreduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n\n# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\nprint(\"Stuff documents using reduce chain...\")\ncombine_documents_chain = StuffDocumentsChain(\n    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n)\n\n# Combines and iteravely reduces the mapped documents\nreduce_documents_chain = ReduceDocumentsChain(\n    # This is final chain that is called.\n    combine_documents_chain=combine_documents_chain,\n    # If documents exceed context for `StuffDocumentsChain`\n    collapse_documents_chain=combine_documents_chain,\n    # The maximum number of tokens to group documents into.\n    token_max=4000\n)\n\n# Combining documents by mapping a chain over them, then combining results\nmap_reduce_chain = MapReduceDocumentsChain(\n    # Map chain\n    llm_chain=map_chain,\n    # Reduce chain\n    reduce_documents_chain=reduce_documents_chain,\n    # The variable name in the llm_chain to put the documents in\n    document_variable_name=\"docs\",\n    # Return the results of the map steps in the output\n    return_intermediate_steps=True,\n    verbose=True\n)\n\n# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n# You might want to play around with different tokenizers and text splitters to see how the results change.\nprint(\"Init chunk splitter...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\") # Hugging face tokenizer for flan-ul2\n    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n        tokenizer=tokenizer\n    )\n    split_docs = text_splitter.split_documents(docs)\n    print(f\"Using {len(split_docs)} chunks: \")\nexcept Exception as ex:\n    print(ex)\n\nprint(\"Run map-reduce chain. This should take ~15-30 seconds...\")\ntry:\n    t1_start = perf_counter()\n    results = map_reduce_chain(split_docs)\n    steps = results[\"intermediate_steps\"]\n    output = results[\"output_text\"]\n    t1_stop = perf_counter()\n    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n\n    print(\"Results from each chunk: \\n\")\n    for idx, step in enumerate(steps):\n        print(f\"{idx + 1}. {step}\\n\")\n    \n    print(\"\\n\\nFinal output:\\n\")\n    print(output)\n\n    print(\"\\nDone.\")\nexcept Exception as e:\n    print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "240c73c2", "cell_type": "markdown", "source": "- As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}