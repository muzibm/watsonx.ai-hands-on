{"cells": [{"metadata": {}, "id": "e53d32ab", "cell_type": "markdown", "source": "# 1. Build AI Apps with RAG using watsonx.ai, LangChain & Vector DB"}, {"metadata": {}, "id": "c64d075d", "cell_type": "markdown", "source": "## Overview\n\nIn this hands-on, you will use LangChain, a framework for building LLM applications.\nYou will learn about:\n- 1.1 Simple Prompt to LLM using LangChain\n- 1.2 Zero-Shot Prompt and Few-Shot Prompt using Prompt Template\n- 1.3 Sequential Prompts using Simple Sequential Chain\n- 1.4 Retrieval Question Answering (QA)\n- 1.5 Documents Summarization"}, {"metadata": {}, "id": "36c33201", "cell_type": "markdown", "source": "## 1.1 Simple Prompt to LLM using LangChain\n- Basic use case of sending prompts to LLM in watsonx (without using Langchain). \n- In this example, we are sending a simple prompt directly to the LLM model (Google flan-ul2)."}, {"metadata": {"scrolled": true}, "id": "1664a584", "cell_type": "code", "source": "# Install library\n!pip install chromadb==0.4.2\n!pip install langchain==0.0.312\n!pip install langchain --upgrade\n!pip install flask-sqlalchemy --user\n!pip install pypdf \n!pip install sentence-transformers\n!pip install langchain_openai", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Collecting chromadb==0.4.2\n  Downloading chromadb-0.4.2-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pandas>=1.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (1.5.3)\nRequirement already satisfied: requests>=2.28 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (2.31.0)\nCollecting pydantic<2.0,>=1.9 (from chromadb==0.4.2)\n  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m150.2/150.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting chroma-hnswlib==0.7.1 (from chromadb==0.4.2)\n  Downloading chroma-hnswlib-0.7.1.tar.gz (30 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.2)\n  Downloading fastapi-0.99.1-py3-none-any.whl.metadata (23 kB)\nCollecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.2)\n  Downloading uvicorn-0.27.1-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: numpy>=1.21.6 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (1.23.5)\nCollecting posthog>=2.4.0 (from chromadb==0.4.2)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nCollecting typing-extensions>=4.5.0 (from chromadb==0.4.2)\n  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.4.2)\n  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb==0.4.2)\n  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nCollecting tokenizers>=0.13.2 (from chromadb==0.4.2)\n  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting pypika>=0.48.9 (from chromadb==0.4.2)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb==0.4.2) (4.65.0)\nCollecting overrides>=7.3.1 (from chromadb==0.4.2)\n  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\nCollecting importlib-resources (from chromadb==0.4.2)\n  Downloading importlib_resources-6.1.2-py3-none-any.whl.metadata (3.9 kB)\nCollecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.2)\n  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.2)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (2.0)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (23.0)\nRequirement already satisfied: protobuf in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (4.21.12)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (1.12)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.4.2) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.4.2) (2022.7)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.2) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.2)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.2)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.2) (2024.2.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.2) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.2) (1.26.18)\nCollecting huggingface_hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb==0.4.2)\n  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: click>=7.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.2) (8.0.4)\nCollecting h11>=0.8 (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.2)\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.2)\n  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.2)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (6.0)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.2)\n  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.2)\n  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.4.2)\n  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.2) (3.9.0)\nCollecting fsspec>=2023.5.0 (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.2)\n  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\nCollecting anyio<5,>=3.4.0 (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.2)\n  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.2)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.2) (1.3.0)\nCollecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.2)\n  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\nCollecting exceptiongroup>=1.0.2 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.2)\n  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\nDownloading chromadb-0.4.2-py3-none-any.whl (399 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m399.3/399.3 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n", "name": "stdout"}, {"output_type": "stream", "text": "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\nDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\nDownloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading importlib_resources-6.1.2-py3-none-any.whl (34 kB)\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading anyio-4.3.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\nDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\nBuilding wheels for collected packages: chroma-hnswlib, pypika\n  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.1-cp310-cp310-linux_x86_64.whl size=181706 sha256=4ccaa32ddf9f5ea84c7ad88d039e86d24071671a9af5f137bb74140ef806435e\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/ad/f2/d2/3f32228e9f4713a9f32a468de8bbc3c642f7805ebef888418b\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=0f25bfdeb9b008c658739e8c7e334a09e6aca01554881fff64f0e305578437c8\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built chroma-hnswlib pypika\nInstalling collected packages: pypika, monotonic, websockets, uvloop, typing-extensions, sniffio, python-dotenv, pulsar-client, overrides, importlib-resources, humanfriendly, httptools, h11, fsspec, exceptiongroup, chroma-hnswlib, backoff, uvicorn, pydantic, posthog, huggingface_hub, coloredlogs, anyio, watchfiles, tokenizers, starlette, onnxruntime, fastapi, chromadb\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.4.0\n    Uninstalling typing_extensions-4.4.0:\n      Successfully uninstalled typing_extensions-4.4.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2022.11.0\n    Uninstalling fsspec-2022.11.0:\n      Successfully uninstalled fsspec-2022.11.0\nSuccessfully installed anyio-4.3.0 backoff-2.2.1 chroma-hnswlib-0.7.1 chromadb-0.4.2 coloredlogs-15.0.1 exceptiongroup-1.2.0 fastapi-0.99.1 fsspec-2024.2.0 h11-0.14.0 httptools-0.6.1 huggingface_hub-0.21.3 humanfriendly-10.0 importlib-resources-6.1.2 monotonic-1.6 onnxruntime-1.17.1 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.4.0 pydantic-1.10.14 pypika-0.48.9 python-dotenv-1.0.1 sniffio-1.3.1 starlette-0.27.0 tokenizers-0.15.2 typing-extensions-4.10.0 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\nCollecting langchain==0.0.312\n  Downloading langchain-0.0.312-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (1.4.39)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (3.9.3)\nCollecting anyio<4.0 (from langchain==0.0.312)\n  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (4.0.2)\nCollecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.312)\n  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\nCollecting jsonpatch<2.0,>=1.33 (from langchain==0.0.312)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting langsmith<0.1.0,>=0.0.43 (from langchain==0.0.312)\n  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (1.23.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (1.10.14)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain==0.0.312) (2.31.0)\nCollecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.312)\n  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.312) (1.8.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.312) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.312) (1.3.1)\nRequirement already satisfied: exceptiongroup in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.312) (1.2.0)\n", "name": "stdout"}, {"output_type": "stream", "text": "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.312)\n  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.312)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.312)\n  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.312) (4.10.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.312) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.312) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.312) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.312) (2.0.1)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.312) (23.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.312) (0.4.3)\nDownloading langchain-0.0.312-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\nDownloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\nDownloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nInstalling collected packages: typing-inspect, tenacity, marshmallow, jsonpointer, anyio, langsmith, jsonpatch, dataclasses-json, langchain\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.0.1\n    Uninstalling tenacity-8.0.1:\n      Successfully uninstalled tenacity-8.0.1\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.3.0\n    Uninstalling anyio-4.3.0:\n      Successfully uninstalled anyio-4.3.0\nSuccessfully installed anyio-3.7.1 dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.312 langsmith-0.0.92 marshmallow-3.21.1 tenacity-8.2.3 typing-inspect-0.9.0\nRequirement already satisfied: langchain in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (0.0.312)\nCollecting langchain\n  Downloading langchain-0.1.11-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (1.4.39)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (3.9.3)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.25 (from langchain)\n  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\nCollecting langchain-core<0.2,>=0.1.29 (from langchain)\n  Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.19-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (1.23.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (1.10.14)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (3.7.1)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.29->langchain)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\nRequirement already satisfied: exceptiongroup in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\n", "name": "stdout"}, {"output_type": "stream", "text": "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\nDownloading langchain-0.1.11-py3-none-any.whl (807 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.29-py3-none-any.whl (252 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.19-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.0\n    Uninstalling packaging-23.0:\n      Successfully uninstalled packaging-23.0\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.0.92\n    Uninstalling langsmith-0.0.92:\n      Successfully uninstalled langsmith-0.0.92\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.0.312\n    Uninstalling langchain-0.0.312:\n      Successfully uninstalled langchain-0.0.312\nSuccessfully installed langchain-0.1.11 langchain-community-0.0.25 langchain-core-0.1.29 langchain-text-splitters-0.0.1 langsmith-0.1.19 orjson-3.9.15 packaging-23.2\nCollecting flask-sqlalchemy\n  Downloading flask_sqlalchemy-3.1.1-py3-none-any.whl.metadata (3.4 kB)\nCollecting flask>=2.2.5 (from flask-sqlalchemy)\n  Downloading flask-3.0.2-py3-none-any.whl.metadata (3.6 kB)\nCollecting sqlalchemy>=2.0.16 (from flask-sqlalchemy)\n  Downloading SQLAlchemy-2.0.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nCollecting Werkzeug>=3.0.0 (from flask>=2.2.5->flask-sqlalchemy)\n  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from flask>=2.2.5->flask-sqlalchemy) (3.1.3)\nCollecting itsdangerous>=2.1.2 (from flask>=2.2.5->flask-sqlalchemy)\n  Downloading itsdangerous-2.1.2-py3-none-any.whl.metadata (2.9 kB)\nCollecting click>=8.1.3 (from flask>=2.2.5->flask-sqlalchemy)\n  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\nCollecting blinker>=1.6.2 (from flask>=2.2.5->flask-sqlalchemy)\n  Downloading blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sqlalchemy>=2.0.16->flask-sqlalchemy) (4.10.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sqlalchemy>=2.0.16->flask-sqlalchemy) (2.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask>=2.2.5->flask-sqlalchemy) (2.1.1)\nDownloading flask_sqlalchemy-3.1.1-py3-none-any.whl (25 kB)\nDownloading flask-3.0.2-py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m101.3/101.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading SQLAlchemy-2.0.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading blinker-1.7.0-py3-none-any.whl (13 kB)\nDownloading click-8.1.7-py3-none-any.whl (97 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\nDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: Werkzeug, sqlalchemy, itsdangerous, click, blinker, flask, flask-sqlalchemy\n\u001b[33m  WARNING: The script flask is installed in '/home/wsuser/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlale 0.7.10 requires click==8.0.4, but you have click 8.1.7 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Werkzeug-3.0.1 blinker-1.7.0 click-8.1.7 flask-3.0.2 flask-sqlalchemy-3.1.1 itsdangerous-2.1.2 sqlalchemy-2.0.28\nCollecting pypdf\n  Downloading pypdf-4.1.0-py3-none-any.whl.metadata (7.4 kB)\nDownloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pypdf\nSuccessfully installed pypdf-4.1.0\nCollecting sentence-transformers\n  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\nCollecting transformers<5.0.0,>=4.32.0 (from sentence-transformers)\n  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers) (2.0.1)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers) (1.1.1)\nRequirement already satisfied: scipy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers) (1.10.1)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers) (0.21.3)\nRequirement already satisfied: Pillow in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sentence-transformers) (10.2.0)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.8.4)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n", "name": "stdout"}, {"output_type": "stream", "text": "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.32.0->sentence-transformers)\n  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\nCollecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.32.0->sentence-transformers)\n  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: joblib>=1.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: safetensors, regex, transformers, sentence-transformers\nSuccessfully installed regex-2023.12.25 safetensors-0.4.2 sentence-transformers-2.5.1 transformers-4.38.2\nCollecting langchain_openai\n  Downloading langchain_openai-0.0.8-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.27 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain_openai) (0.1.29)\nCollecting openai<2.0.0,>=1.10.0 (from langchain_openai)\n  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\nCollecting tiktoken<1,>=0.5.2 (from langchain_openai)\n  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (6.0)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.7.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (0.1.19)\nRequirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.10.14)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (8.2.3)\nCollecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.10.0->langchain_openai)\n  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\nCollecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain_openai)\n  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: sniffio in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.65.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.10.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\nRequirement already satisfied: idna>=2.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.4)\nRequirement already satisfied: exceptiongroup in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.4)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.9.15)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.26.18)\nDownloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\nDownloading openai-1.13.3-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\nDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "name": "stdout"}, {"output_type": "stream", "text": "\u001b[?25hInstalling collected packages: httpcore, distro, tiktoken, httpx, openai, langchain_openai\nSuccessfully installed distro-1.9.0 httpcore-1.0.4 httpx-0.27.0 langchain_openai-0.0.8 openai-1.13.3 tiktoken-0.6.0\n", "name": "stdout"}]}, {"metadata": {"scrolled": true}, "id": "4adcdb35", "cell_type": "code", "source": "# Import libraries\nimport os\nimport warnings\n\n#from dotenv import load_dotenv\nfrom time import sleep\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\nfrom langchain import PromptTemplate # Langchain Prompt Template\nfrom langchain.chains import LLMChain, SimpleSequentialChain # Langchain Chains\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\nfrom langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\nfrom langchain.text_splitter import CharacterTextSplitter # Text splitter\n\nwarnings.filterwarnings(\"ignore\")", "execution_count": 2, "outputs": []}, {"metadata": {}, "id": "96339420", "cell_type": "code", "source": "# Get API key and URL from .env\n#load_dotenv()\napi_key = \"<YOUR API KEY HERE>\"\nibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\nproject_id = \"<YOUR PROJECT ID HERE>\"\n\nif api_key is None or ibm_cloud_url is None or project_id is None:\n    raise Exception(\"One or more environment variables are missing!\")\nelse:\n    creds = {\n        \"url\": ibm_cloud_url,\n        \"apikey\": api_key \n    }", "execution_count": 3, "outputs": []}, {"metadata": {}, "id": "a51cbd27", "cell_type": "code", "source": "# Initialize the watsonx model\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.2,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 25,\n    GenParams.REPETITION_PENALTY: 1.0,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 20\n}\n\nllm_model = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n)\n\nprint(\"Done initializing LLM.\")", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "Done initializing LLM.\n", "name": "stdout"}]}, {"metadata": {}, "id": "1b25c008", "cell_type": "code", "source": "# Send a simple prompt to model\ncountries = [\"France\", \"Japan\", \"Australia\"]\n\ntry:\n  for country in countries:\n    question = f\"What is the capital of {country}\"\n    res = llm_model.generate_text(question)\n    print(f\"The capital of {country} is {res.capitalize()}\")\nexcept Exception as e:\n  print(e)", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "The capital of France is Paris\nThe capital of Japan is Tokyo\nThe capital of Australia is Canberra\n", "name": "stdout"}]}, {"metadata": {}, "id": "0bf74712", "cell_type": "markdown", "source": "## 1.2 Zero-Shot Prompt and Few-Shot Prompt using Prompt Template\n- Real use case can be more complex. Instead of sending plain prompts to LLM, we are using Langchain Prompt Template. \n- In this example, we are using Langchain Prompt Template to send prompt to the LLM model (Google flan-ul2).\n- Advantags of using Prompt Template:\n    1. **Modularity**: With a prompt template, you can define a structured template once and reuse it with different input variables. This makes your code more modular and easier to maintain.\n    2. **Dynamic Input**: Prompt templates allow for dynamic input variables, such as \"country\" in this example. This means you can easily change the input value without modifying the entire prompt structure.\n    3. **Readability**: Templates provide a clear structure for the prompt, making it easier for other developers to understand the purpose of the prompt and how it interacts with the model.\n    4. **Flexibility**: You can customize the template to suit your specific use case or domain requirements. This flexibility enables you to adapt the prompt to different scenarios without rewriting the entire prompt logic."}, {"metadata": {}, "id": "2c92d3c2", "cell_type": "markdown", "source": "## Zero-shot Prompt\n- Zero-shot prompt is the simplest type of prompt. It provides no examples to the model, just the instruction. \n- You can phrase the instruction as a question. i.e: *\"Explain the concept of Generative AI.\"*\n- You can also give the model a 'role'. i.e: *\"You are a Data Scientist. Explain the concept of Generative AI.\"*"}, {"metadata": {"scrolled": true}, "id": "7b140fb9", "cell_type": "code", "source": "# Define the prompt template\nprompt = PromptTemplate(\n  input_variables=[\"country\"],\n  template= \"What is the capital of {country}?\",\n)\n\ntry:\n  # In order to use Langchain, we need to instantiate Langchain extension\n  lc_llm_model = WatsonxLLM(model=llm_model)\n  \n  # Define a chain based on model and prompt\n  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n\n  # Getting predictions\n  countries = [\"France\", \"Japan\", \"Australia\"]\n  for country in countries:\n    response = chain.run(country)\n    print(prompt.format(country=country) + \" = \" + response.capitalize())\n    sleep(0.5)\nexcept Exception as e:\n  print(e)", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "What is the capital of France? = Paris\nWhat is the capital of Japan? = Tokyo\nWhat is the capital of Australia? = Canberra\n", "name": "stdout"}]}, {"metadata": {}, "id": "e52a5340", "cell_type": "markdown", "source": "## Few-shot Prompt\n- Few-shot prompt is giving the model a few examples to figure out how to handle similar task in the future.\n- It helps the model understand the task better."}, {"metadata": {}, "id": "572caefb", "cell_type": "code", "source": "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n\n# Few -shot examples\nexamples = [\n    {\"input\": \"What is the capital of Sweden?\", \"output\": \"Stockholm\"},\n    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"Kuala Lumpur\"},\n]\n\nexample_prompt = ChatPromptTemplate.from_messages(\n    [('human', '{input}'), ('ai', '{output}')]\n)\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n)\n\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [\n        #('system', 'You are a helpful AI Assistant'),\n        few_shot_prompt,\n        ('human', '{input}'),\n    ]\n)", "execution_count": 7, "outputs": []}, {"metadata": {}, "id": "57575ce2", "cell_type": "code", "source": "try:\n  # In order to use Langchain, we need to instantiate Langchain extension\n  lc_llm_model = WatsonxLLM(model=llm_model)\n  \n  # Define a chain based on model and prompt\n  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n\n  # Getting predictions\n  countries = [\"France\", \"Japan\", \"Australia\"]\n  for country in countries:\n    prompt = f\"What is the capital of {country}?\"\n    print(prompt)\n    response = chain.run(prompt)\n    print(response)\n    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n    sleep(0.5)\nexcept Exception as e:\n  print(e)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "What is the capital of France?\nAI: Paris\nWhat is the capital of Japan?\nAI: Tokyo\nWhat is the capital of Australia?\nAI: Canberra\n", "name": "stdout"}]}, {"metadata": {}, "id": "ab2d56c4", "cell_type": "code", "source": "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n\n# Few -shot examples\nexamples = [\n    {\"input\": \"What is the capital of Sweden?\", \"output\": \"The capital of Sweden is Stockholm\"},\n    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"The capital of Malaysia is Kuala Lumpur\"},\n]\n\nexample_prompt = ChatPromptTemplate.from_messages(\n    [('human', '{input}'), ('ai', '{output}')]\n)\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n)\n\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [\n        #('system', 'You are a helpful AI Assistant'),\n        few_shot_prompt,\n        ('human', '{input}'),\n    ]\n)", "execution_count": 9, "outputs": []}, {"metadata": {}, "id": "aeb4bb8c", "cell_type": "code", "source": "try:\n  # In order to use Langchain, we need to instantiate Langchain extension\n  lc_llm_model = WatsonxLLM(model=llm_model)\n  \n  # Define a chain based on model and prompt\n  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n\n  # Getting predictions\n  countries = [\"France\", \"Japan\", \"Australia\"]\n  for country in countries:\n    prompt = f\"What is the capital of {country}?\"\n    print(prompt)\n    response = chain.run(prompt)\n    print(response)\n    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n    sleep(0.5)\nexcept Exception as e:\n  print(e)", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "What is the capital of France?\nAI: The capital of France is Paris\nWhat is the capital of Japan?\nAI: The capital of Japan is Tokyo\nWhat is the capital of Australia?\nAI: The capital of Australia is Canberra\n", "name": "stdout"}]}, {"metadata": {}, "id": "6f4674b8", "cell_type": "markdown", "source": "## 1.3 Sequential Prompts using Simple Sequential Chain\n- By using Simple Sequential Chain in LangChain, you can easily chain multiple prompts to create sequential prompts.\n- Prompt chaining, also known as Sequential prompts, enables the response to one prompt to become the input for the next prompt in the sequence.\n- Each subsequent prompt is informed by the AI's previous response, creating a chain of interactions that progressively refines the model's output.\n- Reference: [SimpleSequentialChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html)"}, {"metadata": {}, "id": "9e999911", "cell_type": "code", "source": "# Create two sequential prompts \npt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\npt2 = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"Answer the following question: {question}\",\n)", "execution_count": 11, "outputs": []}, {"metadata": {}, "id": "5f4aabdc", "cell_type": "code", "source": "# Instantiate 2 models (Note, these could be different models depending on use case)\n# Note the .to_langchain() method which returns a WatsonxLLM wrapper, like above.\nmodel_1 = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()\n\nmodel_2 = Model(\n    model_id=\"google/flan-ul2\",\n    credentials=creds,\n    project_id=project_id\n).to_langchain()", "execution_count": 12, "outputs": []}, {"metadata": {}, "id": "833f356c", "cell_type": "code", "source": "# Construct the sequential chain\nprompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\nprompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\nqa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)", "execution_count": 13, "outputs": []}, {"metadata": {"scrolled": false}, "id": "ed6495db", "cell_type": "code", "source": "# Run our chain with the topic: \"an animal\"\n# Play around with providing different topics to see the output. eg. cars, the Roman empire\ntry:\n  qa.run(\"an animal\")\nexcept Exception as e:\n  print(e)", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "\n\n\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n\u001b[36;1m\u001b[1;3mWhat is the name of the smallest tiger subspecies?\u001b[0m\n\u001b[33;1m\u001b[1;3msaber tiger\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n", "name": "stdout"}]}, {"metadata": {}, "id": "ffd84ba9", "cell_type": "markdown", "source": "## 1.4 Retrieval Question Answering (QA)\n- Using Retrieval Question Answering (QA) in LangChain, you can easily extract passages from documents as answers to your prompt (Question). \n- To begin, download a sample pdf file from this link: [what_is_generative_ai.pdf](https://ibm.box.com/v/what-is-generative-ai)\n- Then, upload your file to Project and create the access token."}, {"metadata": {}, "id": "82e7723f", "cell_type": "code", "source": "# Import library\nfrom ibm_watson_studio_lib import access_project_or_space\nfrom langchain.chains import RetrievalQA", "execution_count": 15, "outputs": []}, {"metadata": {}, "id": "edc8757d", "cell_type": "code", "source": "# Create access token in project\ntoken = \"p-2+quQwUmxbcHBV0uXmErPr4w==;nB+1uZ69L3PBjBC0yKB6dg==:gW2E/+zP0W7AiB6fVRzYR6UkFdzcSfBDXkswZsnxrMXbgw/i+vNFxRVA2EL2iC846UrCDX3uWcANxCjlI1lc6XgVeMXX6Tx21w==\"\nwslib = access_project_or_space({\"token\":token})\nwslib.download_file(\"what_is_generative_ai.pdf\")", "execution_count": 16, "outputs": [{"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "{'file_name': 'what_is_generative_ai.pdf',\n 'summary': ['loaded data', 'saved to file']}"}, "metadata": {}}]}, {"metadata": {"scrolled": true}, "id": "839d0893", "cell_type": "code", "source": "# Load PDF document\npdf = 'what_is_generative_ai.pdf'\nloaders = [PyPDFLoader(pdf)]", "execution_count": 17, "outputs": []}, {"metadata": {"scrolled": false}, "id": "62bb2f6e", "cell_type": "code", "source": "# Index loaded PDF\nindex = VectorstoreIndexCreator(\n    embedding = HuggingFaceEmbeddings(),\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "884f9a96a8e54cfcb584b058758bc1dc"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1eeacfadd4bd4a4b957f2af25ee1b88e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "949fcd50f88741e59748f456c83a769c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "06591ca47eca4290833a580e43063781"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5072c0e982b947f38a65f8019e886fbb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "67047a6d249a4e89854f6859c272bd4a"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b76d06f2baeb4e79a756af32d2f3c61f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b570e349b0aa466795c72075ab0f2c7d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "05980d4c836e4d73bb3107b7026d4fa8"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "815a0bac6a254ff4a987e0264a2b7ef7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "60966d6b36fd4c50af143469f5e9d3d4"}}, "metadata": {}}]}, {"metadata": {}, "id": "e723ff88", "cell_type": "code", "source": "# Initialize watsonx google/flan-ul2 model\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.2,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 100,\n    GenParams.MIN_NEW_TOKENS: 50,\n    GenParams.MAX_NEW_TOKENS: 300\n}\n\nmodel = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()", "execution_count": 19, "outputs": []}, {"metadata": {}, "id": "997ae488", "cell_type": "code", "source": "# Initialize RAG chain\nchain = RetrievalQA.from_chain_type(llm=model, \n                                    chain_type=\"stuff\", \n                                    retriever=index.vectorstore.as_retriever(), \n                                    input_key=\"question\")", "execution_count": 20, "outputs": []}, {"metadata": {}, "id": "d4891cb0", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What is Machine Learning?\")\nprint(res)", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "Machine learning is a type of artificial intelligence. Through machine learning, practitioners develop artificial intelligence through models that can \u201clearn\u201d from data patterns without human direction. The unmanageably huge volume and complexity of data (unmanageable by humans, anyway) that is now being generated has increased the potential of machine learning, as well as the need for it.\n", "name": "stdout"}]}, {"metadata": {}, "id": "20564da9", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What are the problems generative AI can solve?\")\nprint(res)", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "Create new content, including audio, code, images, text, simulations, and videos. It also produced an already famous passage describing how to remove a peanut butter sandwich from a VCR in the style of the King James Bible.\n", "name": "stdout"}]}, {"metadata": {}, "id": "a3ab61fd", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What are the risks of Generative AI?\")\nprint(res)", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "Unknown. 3 What is generative AI? Designed by McKinsey Global Publishing Copyright  2023 McKinsey & Company. All rights reserved.These risks can be mitigated, however, in a few ways. For one, it\u2019s crucial to carefully select the initial data used to train these models to avoid including toxic or biased content. Next, rather than employing an off-the-shelf generative AI model, organizations could consider using smaller, specialized models. Organizations with more resources could also customize a general model based on their own data to fit their needs and minimize biases. Organizations should also keep a human in the loop (that is, to make sure a real human checks the output of a generative AI model before it is published or used) and avoid using generative AI models for critical decisions, such as those involving significant resources or human welfare.\n", "name": "stdout"}]}, {"metadata": {}, "id": "7187e120", "cell_type": "markdown", "source": "## 1.5 Documents Summarization\n- Text summarization is a task in NLP that makes short but informative summaries of long texts. LLM can be used to make summaries of news articles, research papers, technical documents, and other kinds of text.\n- Summarizing long documents can be challenging. To generate summaries, you need to apply summarization strategies on your indexed documents. \n- In this example, we will summarize long documents from these 3 websites:\n     - https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\n     - https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/\n     - https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\n- When building a summarizer app, these are methods to pass your documents into the LLM\u2019s context window:\n    1. **Method 1: Stuff** - Simply \u201cstuff\u201d all documents into a single prompt. (Simplest method)\n    2. **Method 2: MapReduce** - Summarize each document on it\u2019s own in a \u201cmap\u201d step and then \u201creduce\u201d the summaries into a final summary."}, {"metadata": {"scrolled": true}, "id": "c551b02c", "cell_type": "code", "source": "# Install library\n!pip3 install transformers chromadb langchain", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n", "name": "stderr"}, {"output_type": "stream", "text": "Requirement already satisfied: transformers in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (4.38.2)\nRequirement already satisfied: chromadb in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: langchain in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (0.1.11)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (0.21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: pandas>=1.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.5.3)\nRequirement already satisfied: pydantic<2.0,>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.10.14)\nRequirement already satisfied: chroma-hnswlib==0.7.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.7.1)\nRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.99.1)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.27.1)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (4.10.0)\nRequirement already satisfied: pulsar-client>=3.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (3.4.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.17.1)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.48.9)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (6.1.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /home/wsuser/.local/lib/python3.10/site-packages (from langchain) (2.0.28)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (3.9.3)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.25 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.0.25)\nRequirement already satisfied: langchain-core<0.2,>=0.1.29 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.1.29)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.0.1)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.1.19)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (3.7.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\nRequirement already satisfied: coloredlogs in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (2.0)\nRequirement already satisfied: protobuf in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.21.12)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas>=1.3->chromadb) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas>=1.3->chromadb) (2022.7)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb) (2024.2.2)\n", "name": "stdout"}, {"output_type": "stream", "text": "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\nRequirement already satisfied: click>=7.0 in /home/wsuser/.local/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\nRequirement already satisfied: exceptiongroup in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n", "name": "stdout"}]}, {"metadata": {}, "id": "b8903f79", "cell_type": "code", "source": "# Import libraries\nimport os\nfrom dotenv import load_dotenv\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.chains.summarize import load_summarize_chain\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM", "execution_count": 25, "outputs": []}, {"metadata": {}, "id": "0eaaddb9", "cell_type": "markdown", "source": "## Method 1: Stuff\n- This method simply \u201cstuff\u201d all documents into a single prompt.\n- What you need to do is setting `stuff` as `chain_type` of your chain."}, {"metadata": {}, "id": "915fcefa", "cell_type": "markdown", "source": "### Stuff without using Prompt Template\n- Prompt and LLMs pipeline is wrapped in a single object: `load_summarize_chain`.\n- Set `stuff` as the `chain_type`.\n- In this example, you will see that the relatively short document will be summarized successfully."}, {"metadata": {"scrolled": false}, "id": "3dba2d0a", "cell_type": "code", "source": "# Initialize document loader\nloader = WebBaseLoader(\"https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\")\ndoc = loader.load()\n\n# Initialize watsonx google/flan-t5-xxl model\n# You might need to tweak some of the runtime parameters to optimize the results\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.15,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 20,\n    GenParams.REPETITION_PENALTY: 1.0,\n    GenParams.MIN_NEW_TOKENS: 20,\n    GenParams.MAX_NEW_TOKENS: 205\n}\n\nflan_model = Model(\n    model_id=\"google/flan-t5-xxl\", \n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()\n\n# Set chain_type as 'stuff'\nchain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n\n# Run summarization task\nres = chain.run(doc)\nprint(res)", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "The new wave of AI, with foundational models provided by generative AI, could represent the new major opportunity to put AI to work for governments.\n", "name": "stdout"}]}, {"metadata": {}, "id": "eeb53a81", "cell_type": "markdown", "source": "### Stuff using Prompt Template\n- You will load the document into a prompt template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well.\n- `StuffDocumentsChain` will be used as part of the `load_summarize_chain` method.\n- In this example, you will see the same summarization output as above.\n- Reference: [StuffDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.chains.combine_documents.stuff.StuffDocumentsChain)"}, {"metadata": {"scrolled": true}, "id": "6b1c8e1a", "cell_type": "code", "source": "#Import librararies\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\n\n# Define prompt\nprompt_template = \"\"\"Write a concise summary of the following:\n\"{text}\"\nCONCISE SUMMARY:\"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\n\n# Define LLMs chain\nllm_chain = LLMChain(llm=flan_model, prompt=prompt)\n\n# Define StuffDocumentsChain\nstuff_chain = StuffDocumentsChain(\n    llm_chain=llm_chain, document_variable_name=\"text\"\n)\n\n# Run summarization task \nres = stuff_chain.run(doc)\nprint(res)", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "Few technologies have taken the world by storm the way artificial intelligence (AI) has over the past few years. AI and its many use cases have become a topic of public discussion no longer relegated to tech experts. AI\u2014generative AI, in particular\u2014has tremendous potential to transform society as we know it for good, boost productivity and unlock trillions in economic value in the coming years.\n", "name": "stdout"}]}, {"metadata": {}, "id": "2938eb51", "cell_type": "markdown", "source": "### Limitation of 'Stuff' Method due to LLMs token limit\n- In this example, you will see that as we add more documents (which increase the tokens), this error will be raised: `the number of input tokens 5222 cannot exceed the total tokens limit 4096 for this model`\n- This is due to the token limit for the model (Max context window length). \n- With LangChain, this can be worked around by using `MapReduce` which execute chunking and recursive summarization method."}, {"metadata": {}, "id": "43c2879b", "cell_type": "code", "source": "# Load a new document from URL\nloader_2 = WebBaseLoader('https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/')\ndoc_2 = loader_2.load()\n\n# Combine the new document to the previous document\ndocs = doc + doc_2\n\n# Run the stuff chain\ntry:\n  res = stuff_chain.run(docs)\n  print(res)\nexcept Exception as e:\n  print(e)", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-02-07)\nStatus code: 400, body: {\"errors\":[{\"code\":\"invalid_input_argument\",\"message\":\"Invalid input argument for Model 'google/flan-t5-xxl': the number of input tokens 5112 cannot exceed the total tokens limit 4096 for this model\"}],\"trace\":\"114daab5a2495dbbec2392e717d67aef\",\"status_code\":400}\n", "name": "stderr"}, {"output_type": "stream", "text": "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-02-07)\nStatus code: 400, body: {\"errors\":[{\"code\":\"invalid_input_argument\",\"message\":\"Invalid input argument for Model 'google/flan-t5-xxl': the number of input tokens 5112 cannot exceed the total tokens limit 4096 for this model\"}],\"trace\":\"114daab5a2495dbbec2392e717d67aef\",\"status_code\":400}\n", "name": "stdout"}]}, {"metadata": {}, "id": "26d3f5e1", "cell_type": "markdown", "source": "## Method 2: MapReduce\n- This method summarize each document on it\u2019s own in a \u201cmap\u201d step and then \u201creduce\u201d the summaries into a final summary.\n- Reference: [ReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html#langchain.chains.combine_documents.reduce.ReduceDocumentsChain)\n- Reference: [MapReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain)"}, {"metadata": {}, "id": "afc5688e", "cell_type": "code", "source": "from transformers import AutoTokenizer\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\nfrom time import perf_counter\n\n# Add a 3rd document\nprint(\"Loading 3rd document...\")\nloader_3 = WebBaseLoader(\"https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\")\ndoc_3 = loader_3.load()\ndocs = docs + doc_3\n\n# Map\nmap_template = \"\"\"The following is a set of documents\n{docs}\nBased on this list of docs, please identify the main themes \nHelpful Answer:\"\"\"\nmap_prompt = PromptTemplate.from_template(map_template)\nprint(\"Init map chain...\")\nmap_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n\n# Reduce\nreduce_template = \"\"\"The following is set of summaries:\n{doc_summaries}\nTake these and distill it into a final, consolidated summary of the main themes. \nHelpful Answer:\"\"\"\nreduce_prompt = PromptTemplate.from_template(reduce_template)\nprint(\"Init reduce chain...\")\nreduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n\n# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\nprint(\"Stuff documents using reduce chain...\")\ncombine_documents_chain = StuffDocumentsChain(\n    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n)\n\n# Combines and iteravely reduces the mapped documents\nreduce_documents_chain = ReduceDocumentsChain(\n    # This is final chain that is called.\n    combine_documents_chain=combine_documents_chain,\n    # If documents exceed context for `StuffDocumentsChain`\n    collapse_documents_chain=combine_documents_chain,\n    # The maximum number of tokens to group documents into.\n    token_max=4000\n)\n\n# Combining documents by mapping a chain over them, then combining results\nmap_reduce_chain = MapReduceDocumentsChain(\n    # Map chain\n    llm_chain=map_chain,\n    # Reduce chain\n    reduce_documents_chain=reduce_documents_chain,\n    # The variable name in the llm_chain to put the documents in\n    document_variable_name=\"docs\",\n    # Return the results of the map steps in the output\n    return_intermediate_steps=True,\n    verbose=True\n)\n\n# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n# You might want to play around with different tokenizers and text splitters to see how the results change.\nprint(\"Init chunk splitter...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\") # Hugging face tokenizer for flan-ul2\n    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n        tokenizer=tokenizer\n    )\n    split_docs = text_splitter.split_documents(docs)\n    print(f\"Using {len(split_docs)} chunks: \")\nexcept Exception as ex:\n    print(ex)\n\nprint(\"Run map-reduce chain. This should take ~15-30 seconds...\")\ntry:\n    t1_start = perf_counter()\n    results = map_reduce_chain(split_docs)\n    steps = results[\"intermediate_steps\"]\n    output = results[\"output_text\"]\n    t1_stop = perf_counter()\n    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n\n    print(\"Results from each chunk: \\n\")\n    for idx, step in enumerate(steps):\n        print(f\"{idx + 1}. {step}\\n\")\n    \n    print(\"\\n\\nFinal output:\\n\")\n    print(output)\n\n    print(\"\\nDone.\")\nexcept Exception as e:\n    print(e)", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "Loading 3rd document...\nInit map chain...\nInit reduce chain...\nStuff documents using reduce chain...\nInit chunk splitter...\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ec0a4b7a4fdb49ceb16f69be4ccb14ca"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "649f32b07c004388ab5e6c50cb3e7a0e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1944363a04e14a3b9d410556c2a76899"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7958c676d03540ec90d3ff5d9d2c2f66"}}, "metadata": {}}, {"output_type": "stream", "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 512). Running this sequence through the model will result in indexing errors\n", "name": "stderr"}, {"output_type": "stream", "text": "Using 3 chunks: \nRun map-reduce chain. This should take ~15-30 seconds...\n\n\n\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5e3d983d11e04bba9faeb795e3b05568"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "074e3bb42c2e41259ffed713efb1f08d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "37d7f4a22fea4a59833f92d2c3eb55b4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "63b743cea17c4e80ae136958b41d01bf"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8e0a46e8c6a848708757d416f8ef4559"}}, "metadata": {}}, {"output_type": "stream", "text": "\n\u001b[1m> Finished chain.\u001b[0m\nElapsed time: 46.46 seconds.\n\nResults from each chunk: \n\n1. AI\u2014generative AI, in particular\u2014has tremendous potential to transform society as we know it for good, boost productivity and unlock trillions in economic value in the coming years.\n\n2. Federal employees are going to see AI tools show up in cloud-based productivity suites sooner rather than later, but it's not clear yet how the trending tech will impact public-facing digital services. Natalie Alms Staff Reporter, Nextgov/FCW Federal activity in the generative AI space so far has been limited. While federal agencies have fielded more than a thousand AI use cases, they aren't yet widely leveraging the content-creation powers of ChatGPT, Google's Bard and other large language models.Part of the reason could be a lack of direction. The White House announced in May that the Office of Management and Budget plans to unveil policy guidance on the use of AI in the federal government sometime this summer. As of this writing, agencies have the AI Bill of Rights framework and an AI Risk Management Framework from the National Institute of Standards and Technology.\u201cThe reason we aren\u2019t looking at, \u2018Hey, are agencies meeting\n\n3. How government agencies come to use generative AI and other innovative technologies in their operations will largely depend upon how the regulatory scheme unfolds\n\n\n\nFinal output:\n\nThe federal government isn't yet leveraging the content-creation powers of ChatGPT, Google's Bard and other large language models.\n\nDone.\n", "name": "stdout"}]}, {"metadata": {}, "id": "240c73c2", "cell_type": "markdown", "source": "- As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}