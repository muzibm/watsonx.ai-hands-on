{"cells": [{"metadata": {}, "id": "e53d32ab", "cell_type": "markdown", "source": "# 1. Build AI Apps with RAG using LangChain"}, {"metadata": {}, "id": "c64d075d", "cell_type": "markdown", "source": "## Overview\n\nIn this hands-on, you will use LangChain, a framework for building LLM applications.\nYou will learn about:\n- 1.1 Simple Prompt to LLM using LangChain\n- 1.2 Zero-Shot Prompt and Few-Shot Prompt using Prompt Template\n- 1.3 Sequential Prompts using Simple Sequential Chain\n- 1.4 Retrieval Question Answering (QA)\n- 1.5 Summarization"}, {"metadata": {}, "id": "36c33201", "cell_type": "markdown", "source": "## 1.1 Simple Prompt to LLM using LangChain\n- Basic use case of sending prompts to LLM in watsonx (without using Langchain). \n- In this example, we are sending a simple prompt directly to the LLM model (Google flan-ul2)."}, {"metadata": {"scrolled": true}, "id": "1664a584", "cell_type": "code", "source": "# Install library\n!pip install chromadb==0.4.2\n!pip install langchain==0.0.312\n!pip install langchain --upgrade\n!pip install flask-sqlalchemy --user\n!pip install pypdf \n!pip install sentence-transformers\n!pip install langchain_openai", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "id": "4adcdb35", "cell_type": "code", "source": "# Import libraries\nimport os\nimport warnings\n\n#from dotenv import load_dotenv\nfrom time import sleep\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\nfrom langchain import PromptTemplate # Langchain Prompt Template\nfrom langchain.chains import LLMChain, SimpleSequentialChain # Langchain Chains\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\nfrom langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\nfrom langchain.text_splitter import CharacterTextSplitter # Text splitter\n\nwarnings.filterwarnings(\"ignore\")", "execution_count": 1, "outputs": []}, {"metadata": {}, "id": "96339420", "cell_type": "code", "source": "# Get API key and URL from .env\n#load_dotenv()\napi_key = \"d7mduD-J_BxCPq4g9TOSzcnAJ4RnNRghAfRMGO-ustiW\"\nibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\nproject_id = \"bd392b76-c25c-4f65-bcc9-2757619f4fe8\"\n\nif api_key is None or ibm_cloud_url is None or project_id is None:\n    raise Exception(\"One or more environment variables are missing!\")\nelse:\n    creds = {\n        \"url\": ibm_cloud_url,\n        \"apikey\": api_key \n    }", "execution_count": 2, "outputs": []}, {"metadata": {}, "id": "a51cbd27", "cell_type": "code", "source": "# Initialize the watsonx model\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.2,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 25,\n    GenParams.REPETITION_PENALTY: 1.0,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 20\n}\n\nllm_model = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n)\n\nprint(\"Done initializing LLM.\")", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Done initializing LLM.\n", "name": "stdout"}]}, {"metadata": {}, "id": "1b25c008", "cell_type": "code", "source": "# Send a simple prompt to model\ncountries = [\"France\", \"Japan\", \"Australia\"]\n\ntry:\n  for country in countries:\n    question = f\"What is the capital of {country}\"\n    res = llm_model.generate_text(question)\n    print(f\"The capital of {country} is {res.capitalize()}\")\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "0bf74712", "cell_type": "markdown", "source": "## 1.2 Zero-Shot Prompt and Few-Shot Prompt using Prompt Template\n- Real use case can be more complex. Instead of sending plain prompts to LLM, we are using Langchain Prompt Template. \n- In this example, we are using Langchain Prompt Template to send prompt to the LLM model (Google flan-ul2).\n- Advantags of using Prompt Template:\n    1. **Modularity**: With a prompt template, you can define a structured template once and reuse it with different input variables. This makes your code more modular and easier to maintain.\n    2. **Dynamic Input**: Prompt templates allow for dynamic input variables, such as \"country\" in this example. This means you can easily change the input value without modifying the entire prompt structure.\n    3. **Readability**: Templates provide a clear structure for the prompt, making it easier for other developers to understand the purpose of the prompt and how it interacts with the model.\n    4. **Flexibility**: You can customize the template to suit your specific use case or domain requirements. This flexibility enables you to adapt the prompt to different scenarios without rewriting the entire prompt logic."}, {"metadata": {}, "id": "2c92d3c2", "cell_type": "markdown", "source": "## Zero-shot Prompt\n- Zero-shot prompt is the simplest type of prompt. It provides no examples to the model, just the instruction. \n- You can phrase the instruction as a question. i.e: *\"Explain the concept of Generative AI.\"*\n- You can also give the model a 'role'. i.e: *\"You are a Data Scientist. Explain the concept of Generative AI.\"*"}, {"metadata": {"scrolled": true}, "id": "7b140fb9", "cell_type": "code", "source": "# Define the prompt template\nprompt = PromptTemplate(\n  input_variables=[\"country\"],\n  template= \"What is the capital of {country}?\",\n)\n\ntry:\n  # In order to use Langchain, we need to instantiate Langchain extension\n  lc_llm_model = WatsonxLLM(model=llm_model)\n  \n  # Define a chain based on model and prompt\n  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n\n  # Getting predictions\n  countries = [\"France\", \"Japan\", \"Australia\"]\n  for country in countries:\n    response = chain.run(country)\n    print(prompt.format(country=country) + \" = \" + response.capitalize())\n    sleep(0.5)\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "e52a5340", "cell_type": "markdown", "source": "## Few-shot Prompt\n- Few-shot prompt is giving the model a few examples to figure out how to handle similar task in the future.\n- It helps the model understand the task better."}, {"metadata": {}, "id": "572caefb", "cell_type": "code", "source": "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n\n# Few -shot examples\nexamples = [\n    {\"input\": \"What is the capital of Sweden?\", \"output\": \"Stockholm\"},\n    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"Kuala Lumpur\"},\n]\n\nexample_prompt = ChatPromptTemplate.from_messages(\n    [('human', '{input}'), ('ai', '{output}')]\n)\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n)\n\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [\n        #('system', 'You are a helpful AI Assistant'),\n        few_shot_prompt,\n        ('human', '{input}'),\n    ]\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "57575ce2", "cell_type": "code", "source": "try:\n  # In order to use Langchain, we need to instantiate Langchain extension\n  lc_llm_model = WatsonxLLM(model=llm_model)\n  \n  # Define a chain based on model and prompt\n  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n\n  # Getting predictions\n  countries = [\"France\", \"Japan\", \"Australia\"]\n  for country in countries:\n    prompt = f\"What is the capital of {country}?\"\n    print(prompt)\n    response = chain.run(prompt)\n    print(response)\n    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n    sleep(0.5)\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "ab2d56c4", "cell_type": "code", "source": "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n\n# Few -shot examples\nexamples = [\n    {\"input\": \"What is the capital of Sweden?\", \"output\": \"The capital of Sweden is Stockholm\"},\n    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"The capital of Malaysia is Kuala Lumpur\"},\n]\n\nexample_prompt = ChatPromptTemplate.from_messages(\n    [('human', '{input}'), ('ai', '{output}')]\n)\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n)\n\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [\n        #('system', 'You are a helpful AI Assistant'),\n        few_shot_prompt,\n        ('human', '{input}'),\n    ]\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "aeb4bb8c", "cell_type": "code", "source": "try:\n  # In order to use Langchain, we need to instantiate Langchain extension\n  lc_llm_model = WatsonxLLM(model=llm_model)\n  \n  # Define a chain based on model and prompt\n  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n\n  # Getting predictions\n  countries = [\"France\", \"Japan\", \"Australia\"]\n  for country in countries:\n    prompt = f\"What is the capital of {country}?\"\n    print(prompt)\n    response = chain.run(prompt)\n    print(response)\n    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n    sleep(0.5)\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "6f4674b8", "cell_type": "markdown", "source": "## 1.3 Sequential Prompts using Simple Sequential Chain\n- By using Simple Sequential Chain in LangChain, you can easily chain multiple prompts to create sequential prompts.\n- Prompt chaining, also known as Sequential prompts, enables the response to one prompt to become the input for the next prompt in the sequence.\n- Each subsequent prompt is informed by the AI's previous response, creating a chain of interactions that progressively refines the model's output."}, {"metadata": {}, "id": "9e999911", "cell_type": "code", "source": "# Create two sequential prompts \npt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\npt2 = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"Answer the following question: {question}\",\n)", "execution_count": 4, "outputs": []}, {"metadata": {}, "id": "5f4aabdc", "cell_type": "code", "source": "# Instantiate 2 models (Note, these could be different models depending on use case)\n# Note the .to_langchain() method which returns a WatsonxLLM wrapper, like above.\nmodel_1 = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()\n\nmodel_2 = Model(\n    model_id=\"google/flan-ul2\",\n    credentials=creds,\n    project_id=project_id\n).to_langchain()", "execution_count": 5, "outputs": []}, {"metadata": {}, "id": "833f356c", "cell_type": "code", "source": "# Construct the sequential chain\nprompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\nprompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\nqa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)", "execution_count": 6, "outputs": []}, {"metadata": {"scrolled": false}, "id": "ed6495db", "cell_type": "code", "source": "# Run our chain with the topic: \"an animal\"\n# Play around with providing different topics to see the output. eg. cars, the Roman empire\ntry:\n  qa.run(\"an animal\")\nexcept Exception as e:\n  print(e)", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "\n\n\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n\u001b[36;1m\u001b[1;3mWhat is the name of the smallest bird in the world?\u001b[0m\n\u001b[33;1m\u001b[1;3mbee hummingbird\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n", "name": "stdout"}]}, {"metadata": {}, "id": "ffd84ba9", "cell_type": "markdown", "source": "## 1.4 Retrieval Question Answering (QA)\n- Using Retrieval Question Answering (QA) in LangChain, you can easily extract passages from documents as answers to your prompt (Question). \n- To begin, download a sample pdf file from this link: [what_is_generative_ai.pdf](https://ibm.box.com/v/what-is-generative-ai)\n- Then, upload your file to Project and create the access token."}, {"metadata": {}, "id": "82e7723f", "cell_type": "code", "source": "# Import library\nfrom ibm_watson_studio_lib import access_project_or_space\nfrom langchain.chains import RetrievalQA", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "edc8757d", "cell_type": "code", "source": "# Create access token in project\ntoken = \"p-2+quQwUmxbcHBV0uXmErPr4w==;nB+1uZ69L3PBjBC0yKB6dg==:gW2E/+zP0W7AiB6fVRzYR6UkFdzcSfBDXkswZsnxrMXbgw/i+vNFxRVA2EL2iC846UrCDX3uWcANxCjlI1lc6XgVeMXX6Tx21w==\"\nwslib = access_project_or_space({\"token\":token})\nwslib.download_file(\"what_is_generative_ai.pdf\")", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "id": "839d0893", "cell_type": "code", "source": "# Load PDF document\npdf = 'what_is_generative_ai.pdf'\nloaders = [PyPDFLoader(pdf)]", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "id": "62bb2f6e", "cell_type": "code", "source": "# Index loaded PDF\nindex = VectorstoreIndexCreator(\n    embedding = HuggingFaceEmbeddings(),\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "e723ff88", "cell_type": "code", "source": "# Initialize watsonx google/flan-ul2 model\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.2,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 100,\n    GenParams.MIN_NEW_TOKENS: 50,\n    GenParams.MAX_NEW_TOKENS: 300\n}\n\nmodel = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "997ae488", "cell_type": "code", "source": "# Initialize RAG chain\nchain = RetrievalQA.from_chain_type(llm=model, \n                                    chain_type=\"stuff\", \n                                    retriever=index.vectorstore.as_retriever(), \n                                    input_key=\"question\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "d4891cb0", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What is Machine Learning?\")\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "20564da9", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What are the problems generative AI can solve?\")\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "a3ab61fd", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What are the risks of Generative AI?\")\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "7187e120", "cell_type": "markdown", "source": "## 1.5 Summarization\n- Text summarization is a task in NLP that makes short but informative summaries of long texts. LLM can be used to make summaries of news articles, research papers, technical documents, and other kinds of text.\n- Summarizing long documents can be challenging. To generate summaries, you need to apply summarization strategies on your indexed documents. \n- In this example, we will summarize long documents from these 3 websites:\n     - https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\n     - https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/\n     - https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\n- When building a summarizer app, these are methods to pass your documents into the LLM\u2019s context window:\n    1. **Method 1: Stuff** - Simply \u201cstuff\u201d all documents into a single prompt. (Simplest method)\n    2. **Method 2: MapReduce** - Summarize each document on it\u2019s own in a \u201cmap\u201d step and then \u201creduce\u201d the summaries into a final summary."}, {"metadata": {"scrolled": true}, "id": "c551b02c", "cell_type": "code", "source": "# Install library\n!pip3 install transformers chromadb langchain", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "Requirement already satisfied: transformers in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (4.38.1)\nRequirement already satisfied: chromadb in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: langchain in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (0.1.9)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: pandas>=1.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.5.3)\nRequirement already satisfied: pydantic<2.0,>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.10.14)\nRequirement already satisfied: chroma-hnswlib==0.7.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.7.1)\nRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.99.1)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.27.1)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (3.4.2)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (4.10.0)\nRequirement already satisfied: pulsar-client>=3.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (3.4.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (1.17.1)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (0.48.9)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from chromadb) (6.1.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /home/wsuser/.local/lib/python3.10/site-packages (from langchain) (2.0.27)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (3.9.3)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.21 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.0.24)\nRequirement already satisfied: langchain-core<0.2,>=0.1.26 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.1.26)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (0.1.8)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain) (3.7.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain) (3.9.15)\nRequirement already satisfied: coloredlogs in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (2.0)\nRequirement already satisfied: protobuf in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.21.12)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas>=1.3->chromadb) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pandas>=1.3->chromadb) (2022.7)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb) (2024.2.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n", "name": "stdout"}, {"output_type": "stream", "text": "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\nRequirement already satisfied: click>=7.0 in /home/wsuser/.local/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.1)\nRequirement already satisfied: exceptiongroup in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n", "name": "stdout"}]}, {"metadata": {}, "id": "b8903f79", "cell_type": "code", "source": "# Import libraries\nimport os\nfrom dotenv import load_dotenv\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.chains.summarize import load_summarize_chain\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM", "execution_count": 9, "outputs": []}, {"metadata": {}, "id": "0eaaddb9", "cell_type": "markdown", "source": "## Method 1: Stuff\n- This method simply \u201cstuff\u201d all documents into a single prompt.\n- What you need to do is setting `stuff` as `chain_type` of your chain."}, {"metadata": {}, "id": "915fcefa", "cell_type": "markdown", "source": "### Stuff without using Prompt Template\n- Prompt and LLMs pipeline is wrapped in a single object: `load_summarize_chain`.\n- Set `stuff` as the `chain_type`.\n- In this example, you will see that the relatively short document will be summarized successfully."}, {"metadata": {"scrolled": false}, "id": "3dba2d0a", "cell_type": "code", "source": "# Initialize document loader\nloader = WebBaseLoader(\"https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\")\ndoc = loader.load()\n\n# Initialize watsonx google/flan-t5-xxl model\n# You might need to tweak some of the runtime parameters to optimize the results\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.15,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 20,\n    GenParams.REPETITION_PENALTY: 1.0,\n    GenParams.MIN_NEW_TOKENS: 20,\n    GenParams.MAX_NEW_TOKENS: 205\n}\n\nflan_model = Model(\n    model_id=\"google/flan-t5-xxl\", \n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()\n\n# Set chain_type as 'stuff'\nchain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n\n# Run summarization task\nres = chain.run(doc)\nprint(res)", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "Few technologies have taken the world by storm the way artificial intelligence (AI) has over the past few years. AI and its many use cases have become a topic of public discussion no longer relegated to tech experts. AI\u2014generative AI, in particular\u2014has tremendous potential to transform society as we know it for good, boost productivity and unlock trillions in economic value in the coming years.\n", "name": "stdout"}]}, {"metadata": {}, "id": "eeb53a81", "cell_type": "markdown", "source": "### Stuff using Prompt Template\n- You will load the document into a prompt template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well.\n- `StuffDocumentsChain` will be used as part of the `load_summarize_chain` method.\n- In this example, you will see the same summarization output as above."}, {"metadata": {"scrolled": true}, "id": "6b1c8e1a", "cell_type": "code", "source": "#Import librararies\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\n\n# Define prompt\nprompt_template = \"\"\"Write a concise summary of the following:\n\"{text}\"\nCONCISE SUMMARY:\"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\n\n# Define LLMs chain\nllm_chain = LLMChain(llm=flan_model, prompt=prompt)\n\n# Define StuffDocumentsChain\nstuff_chain = StuffDocumentsChain(\n    llm_chain=llm_chain, document_variable_name=\"text\"\n)\n\n# Run summarization task \nres = stuff_chain.run(doc)\nprint(res)", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "Few technologies have taken the world by storm the way artificial intelligence (AI) has over the past few years. AI and its many use cases have become a topic of public discussion no longer relegated to tech experts. AI\u2014generative AI, in particular\u2014has tremendous potential to transform society as we know it for good, boost productivity and unlock trillions in economic value in the coming years. AI\u2019s value is not limited to advances in industry and consumer products alone. When implemented in a responsible way\u2014where the technology is fully governed, privacy is protected and decision making is transparent and explainable\u2014AI has the power to usher in a new era of government services. Such services can empower citizens and help restore trust in public entities by improving workforce efficiency and reducing operational costs in the public sector. On the backend, AI likewise has the potential to supercharge digital modernization by, for example, automating the migration of legacy software to more flexible cloud-based applications,\n", "name": "stdout"}]}, {"metadata": {}, "id": "2938eb51", "cell_type": "markdown", "source": "### Limitation of 'Stuff' Method due to LLMs token limit\n- In this example, you will see that as we add more documents (which increase the tokens), this error will be raised: `the number of input tokens 5222 cannot exceed the total tokens limit 4096 for this model`\n- This is due to the token limit for the model (Max context window length). \n- With LangChain, this can be worked around by using `MapReduce` which execute chunking and recursive summarization method."}, {"metadata": {}, "id": "43c2879b", "cell_type": "code", "source": "# Load a new document from URL\nloader_2 = WebBaseLoader('https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/')\ndoc_2 = loader_2.load()\n\n# Combine the new document to the previous document\ndocs = doc + doc_2\n\n# Run the stuff chain\ntry:\n  res = stuff_chain.run(docs)\n  print(res)\nexcept Exception as e:\n  print(e)", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-02-07)\nStatus code: 400, body: {\"errors\":[{\"code\":\"invalid_input_argument\",\"message\":\"Invalid input argument for Model 'google/flan-t5-xxl': the number of input tokens 5144 cannot exceed the total tokens limit 4096 for this model\"}],\"trace\":\"c2eea31e2b4242f8e79e4e6d796dd542\",\"status_code\":400}\n", "name": "stderr"}, {"output_type": "stream", "text": "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-02-07)\nStatus code: 400, body: {\"errors\":[{\"code\":\"invalid_input_argument\",\"message\":\"Invalid input argument for Model 'google/flan-t5-xxl': the number of input tokens 5144 cannot exceed the total tokens limit 4096 for this model\"}],\"trace\":\"c2eea31e2b4242f8e79e4e6d796dd542\",\"status_code\":400}\n", "name": "stdout"}]}, {"metadata": {}, "id": "26d3f5e1", "cell_type": "markdown", "source": "## Method 2: MapReduce\n- This method summarize each document on it\u2019s own in a \u201cmap\u201d step and then \u201creduce\u201d the summaries into a final summary."}, {"metadata": {}, "id": "afc5688e", "cell_type": "code", "source": "from transformers import AutoTokenizer\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\nfrom time import perf_counter\n\n# Add a 3rd document\nprint(\"Loading 3rd document...\")\nloader_3 = WebBaseLoader(\"https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\")\ndoc_3 = loader_3.load()\ndocs = docs + doc_3\n\n# Map\nmap_template = \"\"\"The following is a set of documents\n{docs}\nBased on this list of docs, please identify the main themes \nHelpful Answer:\"\"\"\nmap_prompt = PromptTemplate.from_template(map_template)\nprint(\"Init map chain...\")\nmap_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n\n# Reduce\nreduce_template = \"\"\"The following is set of summaries:\n{doc_summaries}\nTake these and distill it into a final, consolidated summary of the main themes. \nHelpful Answer:\"\"\"\nreduce_prompt = PromptTemplate.from_template(reduce_template)\nprint(\"Init reduce chain...\")\nreduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n\n# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\nprint(\"Stuff documents using reduce chain...\")\ncombine_documents_chain = StuffDocumentsChain(\n    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n)\n\n# Combines and iteravely reduces the mapped documents\nreduce_documents_chain = ReduceDocumentsChain(\n    # This is final chain that is called.\n    combine_documents_chain=combine_documents_chain,\n    # If documents exceed context for `StuffDocumentsChain`\n    collapse_documents_chain=combine_documents_chain,\n    # The maximum number of tokens to group documents into.\n    token_max=4000\n)\n\n# Combining documents by mapping a chain over them, then combining results\nmap_reduce_chain = MapReduceDocumentsChain(\n    # Map chain\n    llm_chain=map_chain,\n    # Reduce chain\n    reduce_documents_chain=reduce_documents_chain,\n    # The variable name in the llm_chain to put the documents in\n    document_variable_name=\"docs\",\n    # Return the results of the map steps in the output\n    return_intermediate_steps=True,\n    verbose=True\n)\n\n# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n# You might want to play around with different tokenizers and text splitters to see how the results change.\nprint(\"Init chunk splitter...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\") # Hugging face tokenizer for flan-ul2\n    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n        tokenizer=tokenizer\n    )\n    split_docs = text_splitter.split_documents(docs)\n    print(f\"Using {len(split_docs)} chunks: \")\nexcept Exception as ex:\n    print(ex)\n\nprint(\"Run map-reduce chain. This should take ~15-30 seconds...\")\ntry:\n    t1_start = perf_counter()\n    results = map_reduce_chain(split_docs)\n    steps = results[\"intermediate_steps\"]\n    output = results[\"output_text\"]\n    t1_stop = perf_counter()\n    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n\n    print(\"Results from each chunk: \\n\")\n    for idx, step in enumerate(steps):\n        print(f\"{idx + 1}. {step}\\n\")\n    \n    print(\"\\n\\nFinal output:\\n\")\n    print(output)\n\n    print(\"\\nDone.\")\nexcept Exception as e:\n    print(e)", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "Loading 3rd document...\nInit map chain...\nInit reduce chain...\nStuff documents using reduce chain...\nInit chunk splitter...\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "06e79582539845269d43386f4014855d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c36d7b17cd394f56b10261d10b54ae61"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3a072d3da3fe47529a6806a4deedc40e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "65f531fe9b544c969f580a3a1deac63b"}}, "metadata": {}}, {"output_type": "stream", "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 512). Running this sequence through the model will result in indexing errors\n", "name": "stderr"}, {"output_type": "stream", "text": "Using 3 chunks: \nRun map-reduce chain. This should take ~15-30 seconds...\n\n\n\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c6b6b6ebbc7b463db4956aafc217b2c5"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c8c8b7d7de2e4629a6c34d6056cd33d4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6e8ea0dc46a2428ab675936a4a1b3f7e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f5a94a24989844daaf82dcc125152157"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1888bb618e1b4307aedbfbef96ee97a6"}}, "metadata": {}}, {"output_type": "stream", "text": "\n\u001b[1m> Finished chain.\u001b[0m\nElapsed time: 13.3 seconds.\n\nResults from each chunk: \n\n1. AI has the power to usher in a new era of government services. When implemented in a responsible way, AI has the power to usher in a new era of government services.\n\n2. Federal employees are going to see AI tools show up in cloud-based productivity suites sooner rather than later, but it's not clear yet how the trending tech will impact public-facing digital services. Natalie Alms Staff Reporter, Nextgov/FCW Federal activity in the generative AI space so far has been limited. While federal agencies have fielded more than a thousand AI use cases, they aren't yet widely leveraging the content-creation powers of ChatGPT, Google's Bard and other large language models.Part of the reason could be a lack of direction. The White House announced in May that the Office of Management and Budget plans to unveil policy guidance on the use of AI in the federal government sometime this summer. As of this writing, agencies have the AI Bill of Rights framework and an AI Risk Management Framework from the National Institute of Standards and Technology.\u201cThe reason we aren\u2019t looking at, \u2018Hey, are agencies meeting\n\n3. Artificial intelligence (AI) is rapidly becoming an integral part of our daily lives and is expected to be the top factor impacting our professional lives over the next five years, according to Thomson Reuters\u2019 recent Future of Professionals survey report. Of course, this begs the question: Who sets the rules in this newly established playing field?\n\n\n\nFinal output:\n\nWhen implemented in a responsible way, AI has the power to usher in a new era of government services.\n\nDone.\n", "name": "stdout"}]}, {"metadata": {}, "id": "240c73c2", "cell_type": "markdown", "source": "- As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}