{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26bfa626",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) notebook with Watson Discovery and watsonx.ai.\n",
    "\n",
    "This Jupyter Notebook provides an example of how to:\n",
    "\n",
    "1. Create a Watson Discovery collection and upload documents to it.\n",
    "\n",
    "2. Customize this notebook to perform a simple RAG exercise\n",
    "\n",
    "A prompt/query is passed into via this notebook. The code will perform the **Retrieval** task from the document(s) in the Watson Discovery collection. The returned information together with the prmpt to the Large Language Model (LLM) of your choice (as named in the Notebook) to generate the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to uncomment the following line (remove the pound sign **'#'** but leave the exclamation mark **'!'** in) and run this code block once to update to the latest level of ibm-watson. If you do so, comment it out after the update is completed so it will not be executed in future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1345eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ibm-watson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c2b75",
   "metadata": {},
   "source": [
    "## Setting up Watson Discovery and IBM Machine Learning\n",
    "\n",
    "You should not need to modify this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39e97e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from ibm_watson import DiscoveryV2\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b8f3e",
   "metadata": {},
   "source": [
    "## 1. Watson Discovery set up\n",
    "\n",
    "When you set up Watson Discovery, you should have saved the crentials in a file called **ibm-credentials.env**. You will need to use the values from that file. You can open the file using a simple text editor. \n",
    "\n",
    "1. Find the value for **DISCOVERY_APIKEY** from the file and paste it as the value for **IAMAuthenticator** below (between the 2 single quotes).\n",
    "\n",
    "2. Find the value for **DISCOVERY_URL** from the file and paste it as the value for **discovery.set_service_url** below (between the 2 single quotes)\n",
    "\n",
    "This initializes a connection to a Watson Discovery instance with a preloaded pdf document (the IBM Annual Report 2022). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94ab9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticator = IAMAuthenticator('your Watson Discovery API key')\n",
    "authenticator = IAMAuthenticator('NLE4JdLyEsMivduJSXHoP0N1f-8wvYMEEajJ7EbNrh8q')\n",
    "discovery = DiscoveryV2(\n",
    "    version='2020-08-30',\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "#discovery.set_service_url('Your Discovery URL')\n",
    "discovery.set_service_url('https://api.us-south.discovery.watson.cloud.ibm.com/instances/557d1217-fa71-4cb6-8ee7-946b1b81314b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174c40e",
   "metadata": {},
   "source": [
    "## 2. Watson Discovery Search \n",
    "\n",
    "This is a simple question (prompt) that is being posted to the model. This can be surfaced in a Streamlit GUI - which is not the focus of the lab. Clients may have other GUI tools. Here we focus on the underlying Watson Discovery, and later on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eba2a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'I’m interested in IBM’s effect on the environment.  What efforts have they been making in sustainability??'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3023c15",
   "metadata": {},
   "source": [
    "For the block below, you will need to provide the proper information from the Watson Discovery project you created. \n",
    "\n",
    "1. The **Project ID**, paste the value in for **project_id** below (between the 2 single quotes).\n",
    "\n",
    "2. The **Collection ID** (for the collection that includes the IBM Annual Report 2022 report), paste the value in for **collection_ids** below (between the 2 single quotes).\n",
    "\n",
    "There are a few parameters defined for Watson Discovery Search:\n",
    "\n",
    "* **passages.enabled**: A Boolean that specifies whether the service returns a set of the most relevant passage from the documents that were returned by a query that uses the natural_language_query parameter. Discovery uses sophisticated algorithms to determine the best passages of text from all of the documents that are returned by a query. They are displayed as a section within each document query result and are ordered by passage relevance. Including passage retrieval in queries increases the response time because it takes more time to score the passages.\n",
    "\n",
    "* **passages.max_per_document**: One passage is returned per document by default. You can increase the maximum number of passages to return per document by specifying a higher number in the passages.max_per_document parameter.\n",
    "\n",
    "* **find_answers**: By default, Discovery provides answers by returning the entire passage that contains the answer to a natural language query. When the answer-finding feature is enabled, Discovery also provides a \"short answer\" within the passage, and a confidence score to show whether the \"short answer\" answers the question that is explicit or implicit in the user query.\n",
    "\n",
    "* **natural_language_query**: Use a natural language query to enter queries that are expressed in natural language, as might be received from a user in a conversational or free-text interface, such as IBM Watson Assistant. The parameter uses the entire input as the query text. It does not recognize operators. The maximum query string length for a natural language query is 2048.\n",
    "\n",
    "For more details on the query parameters, see https://cloud.ibm.com/docs/discovery-data?topic=discovery-data-query-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba64b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = discovery.query(\n",
    "  project_id='ddd1f9e1-84ea-4f42-bf9f-82cc856a546f',\n",
    "  collection_ids = ['78da73b4-35ca-51a7-0000-018dd1761428'],\n",
    "  passages = {'enabled': True, \n",
    "              'max_per_document': 5,\n",
    "             'find_answers': True},\n",
    "  natural_language_query = question\n",
    ").get_result()\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(response, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6b7c9",
   "metadata": {},
   "source": [
    "The next 4 blocks provide some parsing for the output. You should not need to update these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60e7f453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['matching_results', 'retrieval_details', 'aggregations', 'results'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting the key fields in the WD output\n",
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "787e54bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only one relevant document (because one document in the collection)\n",
    "len(response['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing some tags\n",
    "passages = response['results'][0]['document_passages']\n",
    "passages = [p['passage_text'].replace('<em>', '').replace('</em>', '').replace('\\n','') for p in passages]\n",
    "passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating passages\n",
    "context = '\\n '.join(passages)\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8474f",
   "metadata": {},
   "source": [
    "## 3. Creating Prompt\n",
    "\n",
    "This section creates a prompt with instructions and context to allow the LLM to generate answers based on the passages retrieved by Watson Discovery, and on the rules specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c047d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n",
    "\n",
    "prompt = \\\n",
    "\"<s>[INST] <<SYS>> \\\n",
    "Please answer the following question in one sentence using this text. \\\n",
    "If the question is unanswerable, say 'unanswerable'. \\\n",
    "If you responded to the question, don't say 'unanswerable'. \\\n",
    "Do not include information that's not relevant to the question. \\\n",
    "Do not answer other questions. \\\n",
    "Make sure the language used is English.'\\\n",
    "Do not use repetitions' \\\n",
    "Question:\" + question + \\\n",
    "'<</SYS>>' + context + '[/INST]'\n",
    "\n",
    "#complete_prompt = context + instruction + question\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------\")\n",
    "print(\"*** Prompt:\" + prompt + \"***\")\n",
    "print(\"----------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f237cb",
   "metadata": {},
   "source": [
    "## 4. Configuring watsonx.ai\n",
    "\n",
    "The following section defines the input to the Large Language Model (LLM).  The only item you need to specify is the project_id for watsonx.ai.. Paste the value into **project_id** (between the 2 double quotation marks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2048b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_type,max_tokens,min_tokens,decoding,temperature):#, repetition_penalty):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "    }\n",
    "    \n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": url\n",
    "        },\n",
    "        project_id= \"2081b828-7d15-42ee-9d98-21bdd806fd8a\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf44eba",
   "metadata": {},
   "source": [
    "This section provides the credential for watsonx.ai. \n",
    "\n",
    "1. The watsonx.ai **Project ID** (not the one for Waston Discovery), paste the value into **watsonx_project_id** (between the 2 double quotes).\n",
    "\n",
    "2. The **API Key** (not the one for Watson Discovery), paste the value into **api_key** (between the 2 double quotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed017b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the hosted LLMs is hardcoded because at this time all LLMs share the same endpoint\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "# Replace with your watsonx project id (look up in the project Manage tab)\n",
    "watsonx_project_id = \"2081b828-7d15-42ee-9d98-21bdd806fd8a\"\n",
    "# Replace with your IBM Cloud key\n",
    "api_key = \"n8eRsM2IR52mZmE3TPuhrbAfYgOKymNmVwS83JVHlWYx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9779123",
   "metadata": {},
   "source": [
    "The following block specifies the the specifics for the LLM. In a PoX, you may want to vary these values to show a client how they can get the best results.\n",
    "\n",
    "1. **model_type** specifies the LLM being used. In the example below it is the llama-2-70b-chat model. You can change it to other models. Note that the size of the model will have implications on resource usage. You may wish to try some of the other ones in a PoX and see if they will provide different results. In the block below, there are 4 models (with 3 commented out, so llama2 is being used - you can comment out different ones to try).\n",
    "\n",
    "2. **max_tokens** specifies the maximum number of output tokens. Keep in mind that 1 token does not equal 1 word. In general, you can estimate roughly 3 tokens per word.\n",
    "\n",
    "3. **min_tokens** specifies the minimum number of output tokens.\n",
    "\n",
    "4. **decoding** specifies the decoding method. You can also choose to do **sampling** decoding - in which case you can specify more parameters (such as **max p** and **max k**). More information on these additional parameters can be found from the Watsonx.ai Technical Sales Level 3 class (https://yourlearning.ibm.com/activity/PLAN-182CE6B6CAEA).\n",
    "\n",
    "5. **temperature** specifies how conservative or creative the model will be. The lower it is, the more conservative it it. The range is from 0 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b867491",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "# model_type = \"google/flan-t5-xxl\"\n",
    "# model_type = \"ibm/granite-13b-chat-v1\"\n",
    "# model_type = \"ibm/granite-13b-instruct-v1\"\n",
    "# model_id = \"ibm/mpt-7b-instruct2\"\n",
    "max_tokens = 100\n",
    "min_tokens = 50\n",
    "decoding = DecodingMethods.GREEDY\n",
    "temperature = 0.7\n",
    "\n",
    "# Get the watsonx model\n",
    "model = get_model(model_type, max_tokens, min_tokens, decoding, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b8207",
   "metadata": {},
   "source": [
    "## 5. Answer Generation\n",
    "\n",
    "This block generates the answer based on the input prompt, the specified paramets, and above all the specified Watson Discovery collection of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e049cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_response = model.generate(prompt)\n",
    "response_text = generated_response['results'][0]['generated_text']\n",
    "\n",
    "# print model response\n",
    "print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "print(response_text)\n",
    "print(\"*********************************************************************************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
