{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64d075d",
   "metadata": {},
   "source": [
    "# Hands-on: Build AI Apps with RAG using watsonx.ai, LangChain & Vector Database\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this hands-on, you will use LangChain, a framework for building LLM applications.\n",
    "You will learn about:\n",
    "1. Simple Prompt to LLM using LangChain\n",
    "2. Zero-Shot Prompt and Few-Shot Prompt using Prompt Template\n",
    "3. Sequential Prompts using Simple Sequential Chain\n",
    "4. Retrieval Question Answering (QA)\n",
    "5. Documents Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c33201",
   "metadata": {},
   "source": [
    "## 1. Simple Prompt to LLM (Manually created Prompt Template)\n",
    "- Basic use case of sending prompts to LLM in watsonx (without using Langchain). \n",
    "- In this example, we are sending a simple prompt directly to the LLM model (Google flan-ul2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664a584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install library\n",
    "!pip install chromadb==0.4.2\n",
    "!pip install langchain==0.0.312\n",
    "!pip install langchain --upgrade\n",
    "!pip install flask-sqlalchemy --user\n",
    "!pip install pypdf \n",
    "!pip install sentence-transformers\n",
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adcdb35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from langchain import PromptTemplate # Langchain Prompt Template\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain # Langchain Chains\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\n",
    "from langchain.text_splitter import CharacterTextSplitter # Text splitter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96339420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key and URL from .env\n",
    "#load_dotenv()\n",
    "api_key = \"<YOUR API KEY HERE>\"\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = \"<YOUR PROJECT ID HERE>\"\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the watsonx model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 25,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 20\n",
    "}\n",
    "\n",
    "llm_model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "print(\"Done initializing LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a simple prompt to model\n",
    "countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "\n",
    "try:\n",
    "  for country in countries:\n",
    "    question = f\"What is the capital of {country}\"\n",
    "    res = llm_model.generate_text(question)\n",
    "    print(f\"The capital of {country} is {res.capitalize()}\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf74712",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Prompt and Few-Shot Prompt using LangChain's Prompt Template\n",
    "- Real use case can be more complex. Instead of sending plain prompts to LLM, we are using Langchain Prompt Template. \n",
    "- In this example, we are using Langchain Prompt Template to send prompt to the LLM model (Google flan-ul2).\n",
    "- Advantags of using Prompt Template:\n",
    "    1. **Modularity**: With a prompt template, you can define a structured template once and reuse it with different input variables. This makes your code more modular and easier to maintain.\n",
    "    2. **Dynamic Input**: Prompt templates allow for dynamic input variables, such as \"country\" in this example. This means you can easily change the input value without modifying the entire prompt structure.\n",
    "    3. **Readability**: Templates provide a clear structure for the prompt, making it easier for other developers to understand the purpose of the prompt and how it interacts with the model.\n",
    "    4. **Flexibility**: You can customize the template to suit your specific use case or domain requirements. This flexibility enables you to adapt the prompt to different scenarios without rewriting the entire prompt logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92d3c2",
   "metadata": {},
   "source": [
    "### 2.1 Zero-shot Prompt\n",
    "- Zero-shot prompt is the simplest type of prompt. It provides no examples to the model, just the instruction. \n",
    "- You can phrase the instruction as a question. i.e: *\"Explain the concept of Generative AI.\"*\n",
    "- You can also give the model a 'role'. i.e: *\"You are a Data Scientist. Explain the concept of Generative AI.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b140fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "  input_variables = [\"country\"],\n",
    "  template = \"What is the capital of {country}?\",\n",
    ")\n",
    "\n",
    "try:\n",
    "  # In order to use Langchain, we need to instantiate Langchain extension\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # Define a chain based on model and prompt\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "  for country in countries:\n",
    "    response = chain.run(country)\n",
    "    print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a5340",
   "metadata": {},
   "source": [
    "### 2.2 Few-shot Prompt\n",
    "- Few-shot prompt is giving the model a few examples to figure out how to handle similar task in the future.\n",
    "- It helps the model understand the task better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572caefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Few -shot examples\n",
    "examples = [\n",
    "    {\"input\": \"What is the capital of Sweden?\", \"output\": \"Stockholm\"},\n",
    "    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"Kuala Lumpur\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{input}'), ('ai', '{output}')]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #('system', 'You are a helpful AI Assistant'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57575ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # In order to use Langchain, we need to instantiate Langchain extension\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # Define a chain based on model and prompt\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "  for country in countries:\n",
    "    prompt = f\"What is the capital of {country}?\"\n",
    "    print(prompt)\n",
    "    response = chain.run(prompt)\n",
    "    print(response)\n",
    "    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Few -shot examples\n",
    "examples = [\n",
    "    {\"input\": \"What is the capital of Sweden?\", \"output\": \"The capital of Sweden is Stockholm\"},\n",
    "    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"The capital of Malaysia is Kuala Lumpur\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{input}'), ('ai', '{output}')]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #('system', 'You are a helpful AI Assistant'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # In order to use Langchain, we need to instantiate Langchain extension\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # Define a chain based on model and prompt\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "  for country in countries:\n",
    "    prompt = f\"What is the capital of {country}?\"\n",
    "    print(prompt)\n",
    "    response = chain.run(prompt)\n",
    "    print(response)\n",
    "    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4674b8",
   "metadata": {},
   "source": [
    "## 3. Sequential Prompts using Simple Sequential Chain\n",
    "- By using Simple Sequential Chain in LangChain, you can easily chain multiple prompts to create sequential prompts.\n",
    "- Prompt chaining, also known as Sequential prompts, enables the response to one prompt to become the input for the next prompt in the sequence.\n",
    "- Each subsequent prompt is informed by the AI's previous response, creating a chain of interactions that progressively refines the model's output.\n",
    "- Reference: [SimpleSequentialChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e999911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two sequential prompts \n",
    "pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4aabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate 2 models (Note, these could be different models depending on use case)\n",
    "# Note the .to_langchain() method which returns a WatsonxLLM wrapper, like above.\n",
    "model_1 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "model_2 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the sequential chain\n",
    "prompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\n",
    "prompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6495db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run our chain with the topic: \"an animal\"\n",
    "# Play around with providing different topics to see the output. eg. cars, the Roman empire\n",
    "try:\n",
    "  qa.run(\"an animal\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd84ba9",
   "metadata": {},
   "source": [
    "## 4. Retrieval Question Answering (QA)\n",
    "- Using Retrieval Question Answering (QA) in LangChain, you can easily extract passages from documents as answers to your prompt (Question). \n",
    "- To begin, download a sample pdf file from this link: [what_is_generative_ai.pdf](https://ibm.box.com/v/what-is-generative-ai)\n",
    "- Then, upload your file to Project and create the access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from ibm_watson_studio_lib import access_project_or_space\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create access token in project\n",
    "token = \"<YOUR ACCESS TOKEN HERE>\"\n",
    "wslib = access_project_or_space({\"token\":token})\n",
    "wslib.download_file(\"what_is_generative_ai.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d0893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load PDF document\n",
    "pdf = 'what_is_generative_ai.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb2f6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Index loaded PDF\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding = HuggingFaceEmbeddings(),\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e723ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize watsonx google/flan-ul2 model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.MAX_NEW_TOKENS: 300\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ae488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG chain\n",
    "chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4891cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer based on the document\n",
    "res = chain.run(\"What is Machine Learning?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20564da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer based on the document\n",
    "res = chain.run(\"What are the problems generative AI can solve?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer based on the document\n",
    "res = chain.run(\"What are the risks of Generative AI?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187e120",
   "metadata": {},
   "source": [
    "## 5. Documents Summarization\n",
    "- Text summarization is a task in NLP that makes short but informative summaries of long texts. LLM can be used to make summaries of news articles, research papers, technical documents, and other kinds of text.\n",
    "- Summarizing long documents can be challenging. To generate summaries, you need to apply summarization strategies on your indexed documents. \n",
    "- In this example, we will summarize long documents from these 3 websites:\n",
    "     - https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\n",
    "     - https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/\n",
    "     - https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\n",
    "- When building a summarizer app, these are methods to pass your documents into the LLM’s context window:\n",
    "    1. **Method 1: Stuff** - Simply “stuff” all documents into a single prompt. (Simplest method)\n",
    "    2. **Method 2: MapReduce** - Summarize each document on it’s own in a “map” step and then “reduce” the summaries into a final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551b02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install library\n",
    "!pip3 install transformers chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8903f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaaddb9",
   "metadata": {},
   "source": [
    "### 5.1 Method 1: Stuff\n",
    "- This method simply “stuff” all documents into a single prompt.\n",
    "- What you need to do is setting `stuff` as `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fcefa",
   "metadata": {},
   "source": [
    "### Stuff without using Prompt Template\n",
    "- Prompt and LLMs pipeline is wrapped in a single object: `load_summarize_chain`.\n",
    "- Set `stuff` as the `chain_type`.\n",
    "- In this example, you will see that the relatively short document will be summarized successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba2d0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize document loader\n",
    "loader = WebBaseLoader(\"https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\")\n",
    "doc = loader.load()\n",
    "\n",
    "# Initialize watsonx google/flan-t5-xxl model\n",
    "# You might need to tweak some of the runtime parameters to optimize the results\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.15,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 20,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 20,\n",
    "    GenParams.MAX_NEW_TOKENS: 205\n",
    "}\n",
    "\n",
    "flan_model = Model(\n",
    "    model_id=\"google/flan-t5-xxl\", \n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "# Set chain_type as 'stuff'\n",
    "chain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n",
    "\n",
    "# Run summarization task\n",
    "res = chain.run(doc)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb53a81",
   "metadata": {},
   "source": [
    "### Stuff using Prompt Template\n",
    "- You will load the document into a prompt template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well.\n",
    "- `StuffDocumentsChain` will be used as part of the `load_summarize_chain` method.\n",
    "- In this example, you will see the same summarization output as above.\n",
    "- Reference: [StuffDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.chains.combine_documents.stuff.StuffDocumentsChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c8e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import librararies\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLMs chain\n",
    "llm_chain = LLMChain(llm=flan_model, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "# Run summarization task \n",
    "res = stuff_chain.run(doc)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938eb51",
   "metadata": {},
   "source": [
    "### Limitation of 'Stuff' Method due to LLMs token limit\n",
    "- In this example, you will see that as we add more documents (which increase the tokens), this error will be raised: `the number of input tokens 5222 cannot exceed the total tokens limit 4096 for this model`\n",
    "- This is due to the token limit for the model (Max context window length). \n",
    "- With LangChain, this can be worked around by using `MapReduce` which execute chunking and recursive summarization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a new document from URL\n",
    "loader_2 = WebBaseLoader('https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/')\n",
    "doc_2 = loader_2.load()\n",
    "\n",
    "# Combine the new document to the previous document\n",
    "docs = doc + doc_2\n",
    "\n",
    "# Run the stuff chain\n",
    "try:\n",
    "  res = stuff_chain.run(docs)\n",
    "  print(res)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d3f5e1",
   "metadata": {},
   "source": [
    "### 5.2 Method 2: MapReduce\n",
    "- This method summarize each document on it’s own in a “map” step and then “reduce” the summaries into a final summary.\n",
    "- Reference: [ReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html#langchain.chains.combine_documents.reduce.ReduceDocumentsChain)\n",
    "- Reference: [MapReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from time import perf_counter\n",
    "\n",
    "# Add a 3rd document\n",
    "print(\"Loading 3rd document...\")\n",
    "loader_3 = WebBaseLoader(\"https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\")\n",
    "doc_3 = loader_3.load()\n",
    "docs = docs + doc_3\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "print(\"Init map chain...\")\n",
    "map_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "print(\"Init reduce chain...\")\n",
    "reduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "print(\"Stuff documents using reduce chain...\")\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n",
    "# You might want to play around with different tokenizers and text splitters to see how the results change.\n",
    "print(\"Init chunk splitter...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\") # Hugging face tokenizer for flan-ul2\n",
    "    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    print(f\"Using {len(split_docs)} chunks: \")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print(\"Run map-reduce chain. This should take ~15-30 seconds...\")\n",
    "try:\n",
    "    t1_start = perf_counter()\n",
    "    results = map_reduce_chain(split_docs)\n",
    "    steps = results[\"intermediate_steps\"]\n",
    "    output = results[\"output_text\"]\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n",
    "\n",
    "    print(\"Results from each chunk: \\n\")\n",
    "    for idx, step in enumerate(steps):\n",
    "        print(f\"{idx + 1}. {step}\\n\")\n",
    "    \n",
    "    print(\"\\n\\nFinal output:\\n\")\n",
    "    print(output)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c73c2",
   "metadata": {},
   "source": [
    "- As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
