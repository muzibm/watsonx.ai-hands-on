{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26bfa626",
   "metadata": {},
   "source": [
    "# Build AI Apps with RAG using watsonx.ai\n",
    "\n",
    "## Overview\n",
    "This Jupyter Notebook provides an example of how to:\n",
    "\n",
    "1. Create a Watson Discovery collection and upload documents to it.\n",
    "\n",
    "2. Customize this notebook to perform a simple RAG exercise.\n",
    "\n",
    "A prompt/query is passed into via this notebook. The code will perform the **Retrieval** task from the document(s) in the Watson Discovery collection. The returned information together with the prompt to the Large Language Model (LLM) of your choice (as named in the Notebook) to generate the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1345eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install library\n",
    "!pip install --upgrade ibm-watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e97e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "\n",
    "from ibm_watson import DiscoveryV2\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b8f3e",
   "metadata": {},
   "source": [
    "## 1. Watson Discovery set up\n",
    "\n",
    "When you set up Watson Discovery, you should have saved the credentials in a file called **ibm-credentials.env**. You will need to use the values from that file. You can open the file using a simple text editor. \n",
    "\n",
    "1. Find the value for **DISCOVERY_APIKEY** from the file and paste it as the value for **IAMAuthenticator** below (between the 2 single quotes).\n",
    "\n",
    "2. Find the value for **DISCOVERY_URL** from the file and paste it as the value for **discovery.set_service_url** below (between the 2 single quotes).\n",
    "\n",
    "This initializes a connection to a Watson Discovery instance with a preloaded pdf document (the IBM Annual Report 2022). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Watson Discovery credentials\n",
    "authenticator = IAMAuthenticator('<YOUR WATSON DISCOVERY API KEY HERE>') # DISCOVERY_APIKEY  \n",
    "discovery = DiscoveryV2(\n",
    "    version='2020-08-30',\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "discovery.set_service_url('<YOUR WATSON DISCOVERY URL HERE>') # DISCOVERY_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174c40e",
   "metadata": {},
   "source": [
    "## 2. Watson Discovery Search \n",
    "\n",
    "This is a simple question (prompt) that is being posted to the model. This can be surfaced in a Streamlit GUI - which is not the focus of the lab. Clients may have other GUI tools. Here we focus on the underlying Watson Discovery, and later on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'I’m interested in IBM’s effect on the environment. What efforts have they been making in sustainability?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a3a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = 'I’m interested in IBM’s initiatives on the business in AI. What efforts have they been making in AI?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = 'What is IBM net profit and revenue in 2022?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3023c15",
   "metadata": {},
   "source": [
    "For the block below, you will need to provide the proper information from the Watson Discovery project you created. \n",
    "\n",
    "1. The **Project ID**, paste the value in for **project_id** below (between the 2 single quotes).\n",
    "\n",
    "2. The **Collection ID** (for the collection that includes the IBM Annual Report 2022 report), paste the value in for **collection_ids** below (between the 2 single quotes).\n",
    "\n",
    "There are a few parameters defined for Watson Discovery Search:\n",
    "\n",
    "* **passages.enabled**: A Boolean that specifies whether the service returns a set of the most relevant passage from the documents that were returned by a query that uses the natural_language_query parameter. Watson Discovery uses sophisticated algorithms to determine the best passages of text from all of the documents that are returned by a query. They are displayed as a section within each document query result and are ordered by passage relevance. Including passage retrieval in queries increases the response time because it takes more time to score the passages.\n",
    "\n",
    "* **passages.max_per_document**: One passage is returned per document by default. You can increase the maximum number of passages to return per document by specifying a higher number in the passages.max_per_document parameter.\n",
    "\n",
    "* **find_answers**: By default, Watson Discovery provides answers by returning the entire passage that contains the answer to a natural language query. When the answer-finding feature is enabled, Watson Discovery also provides a \"short answer\" within the passage, and a confidence score to show whether the \"short answer\" answers the question that is explicit or implicit in the user query.\n",
    "\n",
    "* **natural_language_query**: Use a natural language query to enter queries that are expressed in natural language, as might be received from a user in a conversational or free-text interface, such as IBM Watson Assistant. The parameter uses the entire input as the query text. It does not recognize operators. The maximum query string length for a natural language query is 2048.\n",
    "\n",
    "For more details on the query parameters, see https://cloud.ibm.com/docs/discovery-data?topic=discovery-data-query-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize the IBM Watson Discovery service to query a collection for information based on a natural language query\n",
    "response = discovery.query(\n",
    "  project_id='<YOUR PROJECT ID HERE>',\n",
    "  collection_ids = ['<YOUR COLLECTION ID HERE>'],\n",
    "  passages = {'enabled': True, \n",
    "              'max_per_document': 5,\n",
    "             'find_answers': True},\n",
    "  natural_language_query = question\n",
    ").get_result()\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(response, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6b7c9",
   "metadata": {},
   "source": [
    "The next 4 blocks provide some parsing for the output. You should not need to update these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e7f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the key fields in the WD output\n",
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one relevant document (because one document in the collection)\n",
    "len(response['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing some tags\n",
    "passages = response['results'][0]['document_passages']\n",
    "passages = [p['passage_text'].replace('<em>', '').replace('</em>', '').replace('\\n','') for p in passages]\n",
    "passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating passages\n",
    "context = '\\n '.join(passages)\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8474f",
   "metadata": {},
   "source": [
    "## 3. Creating Prompt\n",
    "\n",
    "This section creates a prompt with instructions and context to allow the LLM to generate answers based on the passages retrieved by Watson Discovery, and on the rules specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c047d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n",
    "\n",
    "prompt = \\\n",
    "\"<s>[INST] <<SYS>> \\\n",
    "Please answer the following question in one sentence using this text. \\\n",
    "If the question is unanswerable, say 'unanswerable'. \\\n",
    "If you responded to the question, don't say 'unanswerable'. \\\n",
    "Do not include information that's not relevant to the question. \\\n",
    "Do not answer other questions. \\\n",
    "Make sure the language used is English.'\\\n",
    "Do not use repetitions.' \\\n",
    "Question:\" + question + \\\n",
    "'<</SYS>>' + context + '[/INST]'\n",
    "\n",
    "# complete_prompt = context + instruction + question\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------\")\n",
    "print(\"*** Prompt:\" + prompt + \"***\")\n",
    "print(\"----------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f237cb",
   "metadata": {},
   "source": [
    "## 4. Configuring watsonx.ai\n",
    "\n",
    "The following section defines the input to the Large Language Model (LLM).  The only item you need to specify is the project_id for watsonx.ai.. Paste the value into **project_id** (between the 2 double quotation marks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2048b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the watsonx model\n",
    "def get_model(model_type,max_tokens,min_tokens,decoding,temperature):#, repetition_penalty):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "    }\n",
    "    \n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": url\n",
    "        },\n",
    "        project_id= \"<YOUR WATSONX.AI PROJECT ID HERE>\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf44eba",
   "metadata": {},
   "source": [
    "This section provides the credential for watsonx.ai. \n",
    "\n",
    "1. The watsonx.ai **Project ID** (not the one for Waston Discovery), paste the value into **watsonx_project_id** (between the 2 double quotes).\n",
    "\n",
    "2. The **API Key** (not the one for Watson Discovery), paste the value into **api_key** (between the 2 double quotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed017b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the hosted LLMs is hardcoded because at this time all LLMs share the same endpoint\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "# Replace with your watsonx project id (look up in the project Manage tab)\n",
    "watsonx_project_id = \"<YOUR WATSONX.AI PROJECT ID HERE>\"\n",
    "\n",
    "# Replace with your IBM Cloud key\n",
    "api_key = \"<YOUR IBM CLOUD API KEY HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9779123",
   "metadata": {},
   "source": [
    "The following block specifies the the specifics for the LLM. In a PoX, you may want to vary these values to show a client how they can get the best results.\n",
    "\n",
    "1. **model_type** specifies the LLM being used. In the example below it is the llama-2-70b-chat model. You can change it to other models. Note that the size of the model will have implications on resource usage. You may wish to try some of the other ones in a PoX and see if they will provide different results. In the block below, there are 4 models (with 3 commented out, so llama2 is being used - you can comment out different ones to try).\n",
    "\n",
    "2. **max_tokens** specifies the maximum number of output tokens. Keep in mind that 1 token does not equal 1 word. In general, you can estimate roughly 3 tokens per word.\n",
    "\n",
    "3. **min_tokens** specifies the minimum number of output tokens.\n",
    "\n",
    "4. **decoding** specifies the decoding method. You can also choose to do **sampling** decoding - in which case you can specify more parameters (such as **Top-P** and **Top-K**). More information on these additional parameters can be found from the Watsonx.ai Technical Sales Level 3 class (https://learn.ibm.com/course/view.php?id=13452).\n",
    "\n",
    "5. **temperature** specifies how conservative or creative the model will be. The lower it is, the more conservative it it. The range is from 0 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b867491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up watsonx model and parameters\n",
    "model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "# model_type = \"google/flan-t5-xxl\"\n",
    "# model_type = \"ibm/granite-13b-chat-v1\"\n",
    "# model_type = \"ibm/granite-13b-instruct-v1\"\n",
    "# model_id = \"ibm/mpt-7b-instruct2\"\n",
    "max_tokens = 100\n",
    "min_tokens = 50\n",
    "decoding = DecodingMethods.GREEDY\n",
    "temperature = 0.7\n",
    "\n",
    "# Get the watsonx model\n",
    "model = get_model(model_type, max_tokens, min_tokens, decoding, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b8207",
   "metadata": {},
   "source": [
    "## 5. Answer Generation\n",
    "\n",
    "This block generates the answer based on the input prompt, the specified parameters, and above all the specified Watson Discovery collection of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e049cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a prompt to model\n",
    "generated_response = model.generate(prompt)\n",
    "response_text = generated_response['results'][0]['generated_text']\n",
    "\n",
    "# Print model response\n",
    "print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "print(response_text)\n",
    "print(\"*********************************************************************************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
